{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33274bca-1e97-46c6-8aeb-f60cfbda414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 08:03:38) [Clang 14.0.6 ]\n",
      "NumPy : 1.26.4\n",
      "Pandas: 2.2.3\n",
      "TensorFlow: 2.16.2\n",
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "📄 Loading train.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/train.csv\n",
      "📄 Loading test.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/test.csv\n",
      "📄 Loading sample_submission.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/sample_submission.csv\n",
      "Train shape: (2029, 9)\n",
      "Test  shape: (10, 8)\n",
      "Sample shape: (10, 2)\n",
      "TEXT_COL  = body\n",
      "TARGET_COL= rule_violation\n",
      "ID_COL    = row_id | TARGET_OUT = rule_violation\n",
      "Class weights: {0: 1.0169172932330828, 1: 0.9836363636363636}\n",
      "✅ Loaded engineered features from 03.\n",
      "Features ready. Train: (2029, 42) | Test: (10, 42)\n",
      "[FAST_DEBUG=False] HAS_GPU=True | SEQ_LEN=200 | BATCH=64 | EPOCHS=14\n",
      "Sample batch: (64, 200) (64, 42) (64,)\n",
      "Epoch 1/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - auc: 0.5603 - loss: 0.8179 - prec: 0.5369 - rec: 0.5275\n",
      "Epoch 1: val_loss improved from None to 0.63411, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 458ms/step - auc: 0.5925 - loss: 0.7955 - prec: 0.5653 - rec: 0.5721 - val_auc: 0.7135 - val_loss: 0.6341 - val_prec: 0.6331 - val_rec: 0.7621 - learning_rate: 0.0020\n",
      "Epoch 2/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - auc: 0.6351 - loss: 0.7084 - prec: 0.5820 - rec: 0.5702\n",
      "Epoch 2: val_loss improved from 0.63411 to 0.62622, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 364ms/step - auc: 0.6446 - loss: 0.6991 - prec: 0.6020 - rec: 0.5976 - val_auc: 0.7074 - val_loss: 0.6262 - val_prec: 0.6236 - val_rec: 0.7961 - learning_rate: 0.0020\n",
      "Epoch 3/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - auc: 0.6471 - loss: 0.6936 - prec: 0.6001 - rec: 0.6860\n",
      "Epoch 3: val_loss improved from 0.62622 to 0.61650, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 366ms/step - auc: 0.6553 - loss: 0.6843 - prec: 0.6071 - rec: 0.6388 - val_auc: 0.7138 - val_loss: 0.6165 - val_prec: 0.6261 - val_rec: 0.6990 - learning_rate: 0.0020\n",
      "Epoch 4/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step - auc: 0.6124 - loss: 0.7120 - prec: 0.5759 - rec: 0.5949\n",
      "Epoch 4: val_loss did not improve from 0.61650\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 385ms/step - auc: 0.6320 - loss: 0.7005 - prec: 0.5931 - rec: 0.6061 - val_auc: 0.7146 - val_loss: 0.6198 - val_prec: 0.6115 - val_rec: 0.8252 - learning_rate: 0.0020\n",
      "Epoch 5/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - auc: 0.6620 - loss: 0.6748 - prec: 0.6205 - rec: 0.7175\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.61650\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 345ms/step - auc: 0.6577 - loss: 0.6849 - prec: 0.6172 - rec: 0.6448 - val_auc: 0.6944 - val_loss: 0.6292 - val_prec: 0.6522 - val_rec: 0.6553 - learning_rate: 0.0020\n",
      "Epoch 6/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - auc: 0.6455 - loss: 0.7058 - prec: 0.6175 - rec: 0.6201\n",
      "Epoch 6: val_loss improved from 0.61650 to 0.60923, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 345ms/step - auc: 0.6569 - loss: 0.6921 - prec: 0.6136 - rec: 0.6545 - val_auc: 0.7208 - val_loss: 0.6092 - val_prec: 0.6516 - val_rec: 0.6990 - learning_rate: 0.0010\n",
      "Epoch 7/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - auc: 0.7429 - loss: 0.6096 - prec: 0.6676 - rec: 0.6755\n",
      "Epoch 7: val_loss improved from 0.60923 to 0.56385, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 346ms/step - auc: 0.7572 - loss: 0.6071 - prec: 0.6825 - rec: 0.6958 - val_auc: 0.8171 - val_loss: 0.5639 - val_prec: 0.6892 - val_rec: 0.8398 - learning_rate: 0.0010\n",
      "Epoch 8/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - auc: 0.8638 - loss: 0.5057 - prec: 0.7739 - rec: 0.8273\n",
      "Epoch 8: val_loss did not improve from 0.56385\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 342ms/step - auc: 0.8633 - loss: 0.4974 - prec: 0.8146 - rec: 0.8097 - val_auc: 0.8282 - val_loss: 0.6490 - val_prec: 0.7411 - val_rec: 0.8058 - learning_rate: 0.0010\n",
      "Epoch 9/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - auc: 0.9019 - loss: 0.3970 - prec: 0.8717 - rec: 0.8772\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.56385\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 340ms/step - auc: 0.9012 - loss: 0.4020 - prec: 0.8679 - rec: 0.8836 - val_auc: 0.8048 - val_loss: 0.6837 - val_prec: 0.6965 - val_rec: 0.8689 - learning_rate: 0.0010\n",
      "Epoch 10/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - auc: 0.8758 - loss: 0.4438 - prec: 0.8001 - rec: 0.8835\n",
      "Epoch 10: val_loss did not improve from 0.56385\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 342ms/step - auc: 0.8822 - loss: 0.4296 - prec: 0.8131 - rec: 0.8861 - val_auc: 0.8169 - val_loss: 0.6596 - val_prec: 0.6988 - val_rec: 0.8447 - learning_rate: 5.0000e-04\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "[V1_StackedBiGRU_Attn] SEQ_LEN=200 BATCH=64 drop=0.35 | Val F1(macro)=0.7494 @ thr=0.530\n",
      "Epoch 1/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - auc: 0.5835 - loss: 0.7663 - prec: 0.5676 - rec: 0.5784\n",
      "Epoch 1: val_loss improved from None to 0.62435, saving model to models/V2_CNN_BiGRU_MHA_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 334ms/step - auc: 0.5986 - loss: 0.7550 - prec: 0.5692 - rec: 0.5733 - val_auc: 0.7216 - val_loss: 0.6243 - val_prec: 0.6700 - val_rec: 0.6602 - learning_rate: 0.0020\n",
      "Epoch 2/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - auc: 0.6521 - loss: 0.6879 - prec: 0.6035 - rec: 0.6261\n",
      "Epoch 2: val_loss improved from 0.62435 to 0.59028, saving model to models/V2_CNN_BiGRU_MHA_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 271ms/step - auc: 0.6910 - loss: 0.6597 - prec: 0.6365 - rec: 0.6642 - val_auc: 0.8086 - val_loss: 0.5903 - val_prec: 0.6557 - val_rec: 0.8689 - learning_rate: 0.0020\n",
      "Epoch 3/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - auc: 0.8730 - loss: 0.4782 - prec: 0.7737 - rec: 0.8420\n",
      "Epoch 3: val_loss did not improve from 0.59028\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 269ms/step - auc: 0.8847 - loss: 0.4578 - prec: 0.8019 - rec: 0.8194 - val_auc: 0.8245 - val_loss: 0.5903 - val_prec: 0.8075 - val_rec: 0.6311 - learning_rate: 0.0020\n",
      "Epoch 4/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - auc: 0.9680 - loss: 0.2460 - prec: 0.9274 - rec: 0.8626\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.59028\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 266ms/step - auc: 0.9709 - loss: 0.2249 - prec: 0.9263 - rec: 0.8994 - val_auc: 0.8083 - val_loss: 0.9142 - val_prec: 0.7149 - val_rec: 0.7913 - learning_rate: 0.0020\n",
      "Epoch 5/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - auc: 0.9945 - loss: 0.0997 - prec: 0.9632 - rec: 0.9725\n",
      "Epoch 5: val_loss did not improve from 0.59028\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 265ms/step - auc: 0.9915 - loss: 0.1157 - prec: 0.9646 - rec: 0.9576 - val_auc: 0.7965 - val_loss: 0.9068 - val_prec: 0.7198 - val_rec: 0.7233 - learning_rate: 0.0010\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "[V2_CNN_BiGRU_MHA] SEQ_LEN=200 BATCH=64 drop=0.35 | Val F1(macro)=0.7437 @ thr=0.680\n",
      "Epoch 1/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - auc: 0.5912 - loss: 0.7771 - prec: 0.5656 - rec: 0.5740\n",
      "Epoch 1: val_loss improved from None to 0.61490, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 251ms/step - auc: 0.6004 - loss: 0.7665 - prec: 0.5731 - rec: 0.5939 - val_auc: 0.7147 - val_loss: 0.6149 - val_prec: 0.6810 - val_rec: 0.6942 - learning_rate: 0.0020\n",
      "Epoch 2/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - auc: 0.6294 - loss: 0.7158 - prec: 0.5825 - rec: 0.6024\n",
      "Epoch 2: val_loss did not improve from 0.61490\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 248ms/step - auc: 0.6225 - loss: 0.7266 - prec: 0.5909 - rec: 0.5988 - val_auc: 0.6888 - val_loss: 0.6576 - val_prec: 0.6292 - val_rec: 0.7330 - learning_rate: 0.0020\n",
      "Epoch 3/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - auc: 0.6519 - loss: 0.7003 - prec: 0.6201 - rec: 0.6772\n",
      "Epoch 3: val_loss improved from 0.61490 to 0.61469, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 235ms/step - auc: 0.6350 - loss: 0.7073 - prec: 0.5982 - rec: 0.6388 - val_auc: 0.7157 - val_loss: 0.6147 - val_prec: 0.6331 - val_rec: 0.7621 - learning_rate: 0.0020\n",
      "Epoch 4/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - auc: 0.6385 - loss: 0.7050 - prec: 0.5956 - rec: 0.6387\n",
      "Epoch 4: val_loss improved from 0.61469 to 0.56832, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 230ms/step - auc: 0.6703 - loss: 0.6830 - prec: 0.6200 - rec: 0.6703 - val_auc: 0.7805 - val_loss: 0.5683 - val_prec: 0.6612 - val_rec: 0.7767 - learning_rate: 0.0020\n",
      "Epoch 5/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - auc: 0.7824 - loss: 0.5929 - prec: 0.7374 - rec: 0.7240\n",
      "Epoch 5: val_loss did not improve from 0.56832\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 236ms/step - auc: 0.7782 - loss: 0.6051 - prec: 0.7358 - rec: 0.6921 - val_auc: 0.8316 - val_loss: 0.6708 - val_prec: 0.6538 - val_rec: 0.9078 - learning_rate: 0.0020\n",
      "Epoch 6/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - auc: 0.9068 - loss: 0.4159 - prec: 0.8266 - rec: 0.8538\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.56832\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 236ms/step - auc: 0.9156 - loss: 0.3956 - prec: 0.8719 - rec: 0.8582 - val_auc: 0.8195 - val_loss: 0.5888 - val_prec: 0.7684 - val_rec: 0.7087 - learning_rate: 0.0020\n",
      "Epoch 7/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - auc: 0.9635 - loss: 0.2127 - prec: 0.9653 - rec: 0.9274\n",
      "Epoch 7: val_loss did not improve from 0.56832\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 231ms/step - auc: 0.9677 - loss: 0.2122 - prec: 0.9552 - rec: 0.9297 - val_auc: 0.8149 - val_loss: 0.7581 - val_prec: 0.7919 - val_rec: 0.6650 - learning_rate: 0.0010\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "[V1_StackedBiGRU_Attn] SEQ_LEN=200 BATCH=32 drop=0.35 | Val F1(macro)=0.7302 @ thr=0.625\n",
      "Epoch 1/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - auc: 0.5659 - loss: 0.7770 - prec: 0.5551 - rec: 0.5837\n",
      "Epoch 1: val_loss improved from None to 0.59736, saving model to models/V2_CNN_BiGRU_MHA_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 218ms/step - auc: 0.6195 - loss: 0.7283 - prec: 0.5847 - rec: 0.6315 - val_auc: 0.7661 - val_loss: 0.5974 - val_prec: 0.6192 - val_rec: 0.8447 - learning_rate: 0.0020\n",
      "Epoch 2/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - auc: 0.7471 - loss: 0.6150 - prec: 0.6658 - rec: 0.7224\n",
      "Epoch 2: val_loss improved from 0.59736 to 0.56248, saving model to models/V2_CNN_BiGRU_MHA_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 199ms/step - auc: 0.7868 - loss: 0.5838 - prec: 0.7123 - rec: 0.7442 - val_auc: 0.8042 - val_loss: 0.5625 - val_prec: 0.7371 - val_rec: 0.6942 - learning_rate: 0.0020\n",
      "Epoch 3/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - auc: 0.9314 - loss: 0.3492 - prec: 0.8688 - rec: 0.8382\n",
      "Epoch 3: val_loss did not improve from 0.56248\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 220ms/step - auc: 0.9300 - loss: 0.3558 - prec: 0.8599 - rec: 0.8558 - val_auc: 0.8254 - val_loss: 0.6428 - val_prec: 0.6917 - val_rec: 0.8495 - learning_rate: 0.0020\n",
      "Epoch 4/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - auc: 0.9905 - loss: 0.1454 - prec: 0.9473 - rec: 0.9672\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.56248\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 201ms/step - auc: 0.9876 - loss: 0.1514 - prec: 0.9520 - rec: 0.9612 - val_auc: 0.7896 - val_loss: 0.8966 - val_prec: 0.7059 - val_rec: 0.7573 - learning_rate: 0.0020\n",
      "Epoch 5/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - auc: 0.9954 - loss: 0.0800 - prec: 0.9851 - rec: 0.9757\n",
      "Epoch 5: val_loss did not improve from 0.56248\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 190ms/step - auc: 0.9926 - loss: 0.0953 - prec: 0.9805 - rec: 0.9745 - val_auc: 0.7792 - val_loss: 0.9231 - val_prec: 0.7129 - val_rec: 0.6990 - learning_rate: 0.0010\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "[V2_CNN_BiGRU_MHA] SEQ_LEN=200 BATCH=32 drop=0.3 | Val F1(macro)=0.7284 @ thr=0.410\n",
      "\n",
      "=== BEST CONFIG ===\n",
      "{'variant': 'V1_StackedBiGRU_Attn', 'f1': 0.749368929961922, 'thr': 0.5299999999999999, 'seq_len': 200, 'batch': 64, 'dropout': 0.35, 'model': <Functional name=V1_StackedBiGRU_Attn, built=True>}\n",
      "Predicting test with best model …\n",
      "✅ Saved local copy : submissions/submission.csv\n",
      "\n",
      "Model used: V1_StackedBiGRU_Attn\n",
      "Validation F1 (macro): 0.7494 at threshold 0.530\n",
      "Final submission head:\n",
      "    row_id  rule_violation\n",
      "0    2029               0\n",
      "1    2030               1\n",
      "2    2031               0\n",
      "3    2032               1\n",
      "4    2033               1\n"
     ]
    }
   ],
   "source": [
    "# 06_neural_advanced.ipynb — Task 3.2\n",
    "# Advanced neural variants (Keras) with dual-run support (local + Kaggle)\n",
    "# - Loads data (auto-detect)\n",
    "# - Reuses engineered features from 03_feature_engineering\n",
    "# - Two model variants: Stacked BiGRU + Attention, and CNN+BiGRU+MultiHeadAttention\n",
    "# - Mini hyperparam sweep (seq_len, batch, dropout)\n",
    "# - Threshold tune (macro-F1), write Kaggle /kaggle/working/submission.csv and local submissions/submission.csv\n",
    "\n",
    "# ========= 0) Imports & environment =========\n",
    "import os, sys, re, glob, json, math, warnings\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"NumPy :\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "KAGGLE_DIR = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "KAGGLE_WORKING = \"/kaggle/working\" if IS_KAGGLE else None\n",
    "OUT_KAGGLE = os.path.join(KAGGLE_WORKING, \"submission.csv\") if IS_KAGGLE else None\n",
    "\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "OUT_LOCAL = \"submissions/submission.csv\"\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# ========= 1) Require TensorFlow (Kaggle TF image or local TF install) =========\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    print(\"TensorFlow:\", tf.__version__)\n",
    "    print(\"Devices:\", tf.config.list_physical_devices())\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"TensorFlow is required for Task 3.2.\\n\"\n",
    "        \"On Kaggle: set Image=TensorFlow, Accelerator=None/CPU is fine. Internet OFF.\\n\"\n",
    "        \"On Mac (Apple Silicon): use tensorflow-macos 2.16.x + tensorflow-metal.\\n\"\n",
    "        f\"Import error: {e}\"\n",
    "    )\n",
    "\n",
    "# Optional: quieter logs\n",
    "import logging\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\",\"2\")\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.config.set_soft_device_placement(True)\n",
    "\n",
    "# ========= 2) Data loaders (Kaggle-first, then local fallbacks) =========\n",
    "CANDIDATE_DIRS = [\n",
    "    \".\", \"data/raw\", \"../data/raw\", \"../../data/raw\",\n",
    "    \"jigsaw-agile-community-rules\", \"../jigsaw-agile-community-rules\", \"../../jigsaw-agile-community-rules\"\n",
    "]\n",
    "def _candidate_paths(filename: str):\n",
    "    paths = []\n",
    "    if os.path.exists(KAGGLE_DIR):\n",
    "        paths.append(os.path.join(KAGGLE_DIR, filename))\n",
    "    for d in CANDIDATE_DIRS:\n",
    "        paths.append(os.path.join(d, filename))\n",
    "    paths.extend(glob.glob(f\"**/{filename}\", recursive=True))\n",
    "    seen, out = set(), []\n",
    "    for p in paths:\n",
    "        ap = os.path.abspath(p)\n",
    "        if ap not in seen and os.path.exists(ap):\n",
    "            seen.add(ap); out.append(ap)\n",
    "    return out\n",
    "\n",
    "def read_first_csv(filename: str):\n",
    "    found = _candidate_paths(filename)\n",
    "    if not found:\n",
    "        raise FileNotFoundError(f\"Could not find {filename} in Kaggle folder or local fallbacks.\")\n",
    "    print(f\"📄 Loading {filename} from: {found[0]}\")\n",
    "    return pd.read_csv(found[0])\n",
    "\n",
    "train_df = read_first_csv(\"train.csv\")\n",
    "test_df  = read_first_csv(\"test.csv\")\n",
    "sample   = read_first_csv(\"sample_submission.csv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test  shape:\", test_df.shape)\n",
    "print(\"Sample shape:\", sample.shape)\n",
    "\n",
    "# ========= 3) Column detection =========\n",
    "TEXT_COL = next((c for c in [\"comment_text\",\"body\",\"text\"] if c in train_df.columns), None)\n",
    "TARGET_COL = next((c for c in [\"rule_violation\",\"target\",\"label\"] if c in train_df.columns), None)\n",
    "ID_COL, TARGET_OUT = sample.columns[0], sample.columns[1]\n",
    "assert TEXT_COL and TARGET_COL, \"Expected text and target columns.\"\n",
    "\n",
    "print(f\"TEXT_COL  = {TEXT_COL}\")\n",
    "print(f\"TARGET_COL= {TARGET_COL}\")\n",
    "print(f\"ID_COL    = {ID_COL} | TARGET_OUT = {TARGET_OUT}\")\n",
    "\n",
    "# ========= 4) Data prep & split =========\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "X_text = train_df[TEXT_COL].fillna(\"\").astype(str).values\n",
    "y = train_df[TARGET_COL].astype(int).values\n",
    "X_test_text = test_df[TEXT_COL].fillna(\"\").astype(str).values\n",
    "\n",
    "X_tr_text, X_va_text, y_tr, y_va = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "def compute_class_weights(y_array):\n",
    "    n = len(y_array); pos = int((y_array==1).sum()); neg = n - pos\n",
    "    return {0: n/(2*neg), 1: n/(2*pos)}\n",
    "CLASS_WEIGHTS = compute_class_weights(y_tr)\n",
    "print(\"Class weights:\", CLASS_WEIGHTS)\n",
    "\n",
    "# ========= 5) Load engineered features from Task 2 (or compute if missing) =========\n",
    "import os, re, numpy as np, pandas as pd\n",
    "\n",
    "# write to both local project path and Kaggle-working so later notebooks can see them\n",
    "PKL_DIRS = [\"data/processed\", \"/kaggle/working/data/processed\"] if os.path.exists(\"/kaggle\") else [\"data/processed\"]\n",
    "for d in PKL_DIRS: os.makedirs(d, exist_ok=True)\n",
    "\n",
    "TR_FEATS_PKL = \"data/processed/train_features.pkl\"\n",
    "TE_FEATS_PKL = \"data/processed/test_features.pkl\"\n",
    "\n",
    "def _extract_features_minimal(df, text_col):\n",
    "    s = df[text_col].fillna(\"\").astype(str)\n",
    "    # helpers\n",
    "    def ratio_safe(num, den):\n",
    "        den = np.maximum(den, 1)\n",
    "        return num / den\n",
    "\n",
    "    # counts\n",
    "    char_count = s.str.len().astype(np.int32)\n",
    "    word_count = s.str.split().str.len().fillna(0).astype(np.int32)\n",
    "    uniq_word_count = s.apply(lambda x: len(set(x.split())) if x else 0).astype(np.int32)\n",
    "    avg_word_len = ratio_safe(char_count, word_count).astype(np.float32)\n",
    "\n",
    "    # casings & punctuation\n",
    "    caps_count = s.str.count(r\"[A-Z]\")\n",
    "    caps_ratio = ratio_safe(caps_count, char_count).astype(np.float32)\n",
    "    all_caps_words = s.str.count(r\"\\b[A-Z]{2,}\\b\").astype(np.int32)\n",
    "\n",
    "    excl_count = s.str.count(\"!\")\n",
    "    ques_count = s.str.count(r\"\\?\")\n",
    "    dots3_count = s.str.count(r\"\\.\\.\\.\")\n",
    "    punct_count = s.str.count(r\"[^\\w\\s]\")  # crude punctuation\n",
    "    punct_ratio = ratio_safe(punct_count, char_count).astype(np.float32)\n",
    "    excl_ratio = ratio_safe(excl_count, char_count).astype(np.float32)\n",
    "    ques_ratio = ratio_safe(ques_count, char_count).astype(np.float32)\n",
    "\n",
    "    # reddit / markup / links / quotes\n",
    "    has_user = s.str.contains(r\"u/\\w+\", regex=True).astype(np.int8)\n",
    "    has_sub  = s.str.contains(r\"r/\\w+\", regex=True).astype(np.int8)\n",
    "    has_url  = s.str.contains(r\"http\", regex=True).astype(np.int8)\n",
    "    quote_count = s.str.count(r\"^>|\\n>\", flags=re.MULTILINE).astype(np.int32)\n",
    "    md_links = s.str.count(r\"\\[[^\\]]+\\]\\([^)]+\\)\").astype(np.int32)\n",
    "\n",
    "    # lexical tokens\n",
    "    you_count = s.str.count(r\"\\byou\\b\", flags=re.IGNORECASE).astype(np.int32)\n",
    "    i_count   = s.str.count(r\"\\bi\\b\", flags=re.IGNORECASE).astype(np.int32)\n",
    "    num_count = s.str.count(r\"\\d\").astype(np.int32)\n",
    "    negate_count = s.str.count(r\"\\b(no|not|never|n't)\\b\", flags=re.IGNORECASE).astype(np.int32)\n",
    "\n",
    "    you_ratio = ratio_safe(you_count, word_count).astype(np.float32)\n",
    "    i_ratio   = ratio_safe(i_count, word_count).astype(np.float32)\n",
    "    num_ratio = ratio_safe(num_count, word_count).astype(np.float32)\n",
    "\n",
    "    # lexical diversity (unique / total)\n",
    "    lexical_diversity = ratio_safe(uniq_word_count, word_count).astype(np.float32)\n",
    "\n",
    "    feats = pd.DataFrame({\n",
    "        \"char_count\": char_count,\n",
    "        \"word_count\": word_count,\n",
    "        \"uniq_word_count\": uniq_word_count,\n",
    "        \"avg_word_len\": avg_word_len,\n",
    "        \"caps_ratio\": caps_ratio,\n",
    "        \"all_caps_words\": all_caps_words,\n",
    "        \"excl_count\": excl_count,\n",
    "        \"ques_count\": ques_count,\n",
    "        \"dots3_count\": dots3_count,\n",
    "        \"punct_count\": punct_count,\n",
    "        \"punct_ratio\": punct_ratio,\n",
    "        \"excl_ratio\": excl_ratio,\n",
    "        \"ques_ratio\": ques_ratio,\n",
    "        \"has_user_mention\": has_user,\n",
    "        \"has_subreddit_mention\": has_sub,\n",
    "        \"has_url\": has_url,\n",
    "        \"quote_count\": quote_count,\n",
    "        \"md_links\": md_links,\n",
    "        \"you_count\": you_count,\n",
    "        \"i_count\": i_count,\n",
    "        \"num_count\": num_count,\n",
    "        \"negate_count\": negate_count,\n",
    "        \"you_ratio\": you_ratio,\n",
    "        \"i_ratio\": i_ratio,\n",
    "        \"num_ratio\": num_ratio,\n",
    "        \"lexical_diversity\": lexical_diversity,\n",
    "    }).astype({\n",
    "        # keep dtypes tight where possible\n",
    "        \"all_caps_words\":\"int32\",\"excl_count\":\"int32\",\"ques_count\":\"int32\",\"dots3_count\":\"int32\",\n",
    "        \"punct_count\":\"int32\",\"quote_count\":\"int32\",\"md_links\":\"int32\",\"you_count\":\"int32\",\n",
    "        \"i_count\":\"int32\",\"num_count\":\"int32\",\"negate_count\":\"int32\",\n",
    "        \"has_user_mention\":\"int8\",\"has_subreddit_mention\":\"int8\",\"has_url\":\"int8\",\n",
    "    })\n",
    "\n",
    "    # Ensure consistent column order\n",
    "    return feats.reindex(sorted(feats.columns), axis=1)\n",
    "\n",
    "# Load or compute\n",
    "if os.path.exists(TR_FEATS_PKL) and os.path.exists(TE_FEATS_PKL):\n",
    "    train_features = pd.read_pickle(TR_FEATS_PKL)\n",
    "    test_features  = pd.read_pickle(TE_FEATS_PKL)\n",
    "    print(\"✅ Loaded engineered features from 03.\")\n",
    "else:\n",
    "    print(\"⚙️  Computing engineered features (03 fallback)…\")\n",
    "    train_features = _extract_features_minimal(train_df, TEXT_COL)\n",
    "    test_features  = _extract_features_minimal(test_df,  TEXT_COL)\n",
    "\n",
    "    # Save to both places so future runs find them\n",
    "    for base in PKL_DIRS:\n",
    "        pf_tr = os.path.join(base, \"train_features.pkl\")\n",
    "        pf_te = os.path.join(base, \"test_features.pkl\")\n",
    "        train_features.to_pickle(pf_tr)\n",
    "        test_features.to_pickle(pf_te)\n",
    "        print(\"💾 Saved:\", pf_tr, \"and\", pf_te)\n",
    "\n",
    "print(f\"Features ready. Train: {train_features.shape} | Test: {test_features.shape}\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "feat_scaler = StandardScaler()\n",
    "feats_tr_all = feat_scaler.fit_transform(train_features.values.astype(np.float32))\n",
    "feats_te     = feat_scaler.transform(test_features.values.astype(np.float32))\n",
    "\n",
    "# keep split consistent\n",
    "X_idx = np.arange(len(X_text))\n",
    "idx_tr, idx_va, _, _ = train_test_split(X_idx, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "feats_tr_split = feats_tr_all[idx_tr]\n",
    "feats_va_split = feats_tr_all[idx_va]\n",
    "\n",
    "# ========= 6) FAST_DEBUG toggle + TextVectorization =========\n",
    "FAST_DEBUG = False  # set True to smoke-test quickly\n",
    "MAX_TOKENS = 30000\n",
    "SEQ_LEN = 160 if FAST_DEBUG else 200\n",
    "HAS_GPU = bool(tf.config.list_physical_devices('GPU'))\n",
    "BATCH = 32 if FAST_DEBUG or not HAS_GPU else 64\n",
    "EPOCHS = 4 if FAST_DEBUG else 14  # early stopping will cap earlier\n",
    "\n",
    "print(f\"[FAST_DEBUG={FAST_DEBUG}] HAS_GPU={HAS_GPU} | SEQ_LEN={SEQ_LEN} | BATCH={BATCH} | EPOCHS={EPOCHS}\")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "text_vec = keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS, output_mode=\"int\", output_sequence_length=SEQ_LEN,\n",
    "    standardize=\"lower_and_strip_punctuation\", split=\"whitespace\"\n",
    ")\n",
    "text_vec.adapt(tf.data.Dataset.from_tensor_slices(X_tr_text).batch(256))\n",
    "\n",
    "# ========= 7) tf.data with auxiliary features =========\n",
    "def make_ds_with_feats(x_arr, feats_arr, y_arr=None, train=False):\n",
    "    if y_arr is None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((x_arr, feats_arr))\n",
    "        if train: ds = ds.shuffle(len(x_arr), seed=SEED)\n",
    "        ds = ds.batch(BATCH).map(lambda txt,f: (text_vec(txt), f), num_parallel_calls=AUTOTUNE)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((x_arr, feats_arr, y_arr))\n",
    "        if train: ds = ds.shuffle(len(x_arr), seed=SEED)\n",
    "        ds = ds.batch(BATCH).map(lambda txt,f,y: ((text_vec(txt), f), y), num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "ds_tr = make_ds_with_feats(X_tr_text, feats_tr_split, y_tr, train=True)\n",
    "ds_va = make_ds_with_feats(X_va_text, feats_va_split, y_va, train=False)\n",
    "\n",
    "# sanity\n",
    "for batch in ds_tr.take(1):\n",
    "    (tok_ids, aux_feats), yb = batch\n",
    "    print(\"Sample batch:\", tok_ids.shape, aux_feats.shape, yb.shape)\n",
    "\n",
    "# ========= 8) Model builders (two variants) =========\n",
    "L2 = keras.regularizers.l2(1e-5)\n",
    "\n",
    "def build_v1_stacked_bigru_attn(vocab_size, n_aux, embed_dim=128, gru_units=96, dropout=0.35):\n",
    "    tok_in = keras.Input(shape=(SEQ_LEN,), dtype=\"int32\", name=\"tok_ids\")\n",
    "    aux_in = keras.Input(shape=(n_aux,), dtype=\"float32\", name=\"aux_feats\")\n",
    "\n",
    "    emb = keras.layers.Embedding(vocab_size, embed_dim, mask_zero=False,\n",
    "                                 embeddings_regularizer=L2, name=\"emb\")(tok_in)\n",
    "\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(gru_units, return_sequences=True, dropout=0.2), name=\"bigru_1\")(emb)\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(gru_units//2, return_sequences=True, dropout=0.2), name=\"bigru_2\")(x)\n",
    "\n",
    "    # Lightweight attention: Query from pooled context over Keys/Values = x\n",
    "    q = keras.layers.GlobalAveragePooling1D(name=\"attn_query_pool\")(x)\n",
    "    q = keras.layers.Reshape((1, q.shape[-1]))(q)\n",
    "    attn_out = keras.layers.Attention(name=\"attn\")([q, x])  # shape (None,1,d)\n",
    "    attn_out = keras.layers.Flatten()(attn_out)\n",
    "\n",
    "    aux_h = keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=L2)(aux_in)\n",
    "    aux_h = keras.layers.Dropout(0.2)(aux_h)\n",
    "\n",
    "    h = keras.layers.Concatenate(name=\"fusion\")([attn_out, aux_h])\n",
    "    h = keras.layers.Dropout(dropout)(h)\n",
    "    h = keras.layers.Dense(192, activation=\"relu\", kernel_regularizer=L2)(h)\n",
    "    h = keras.layers.Dropout(dropout)(h)\n",
    "    out = keras.layers.Dense(1, activation=\"sigmoid\", name=\"out\")(h)\n",
    "\n",
    "    model = keras.Model([tok_in, aux_in], out, name=\"V1_StackedBiGRU_Attn\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(2e-3),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[keras.metrics.AUC(name=\"auc\"),\n",
    "                           keras.metrics.Precision(name=\"prec\"),\n",
    "                           keras.metrics.Recall(name=\"rec\")])\n",
    "    return model\n",
    "\n",
    "def build_v2_cnn_bigru_mha(vocab_size, n_aux, embed_dim=128, gru_units=96, dropout=0.35, heads=4):\n",
    "    tok_in = keras.Input(shape=(SEQ_LEN,), dtype=\"int32\", name=\"tok_ids\")\n",
    "    aux_in = keras.Input(shape=(n_aux,), dtype=\"float32\", name=\"aux_feats\")\n",
    "\n",
    "    emb = keras.layers.Embedding(vocab_size, embed_dim, mask_zero=False,\n",
    "                                 embeddings_regularizer=L2, name=\"emb\")(tok_in)\n",
    "\n",
    "    # CNN path\n",
    "    c = keras.layers.SpatialDropout1D(0.2)(emb)\n",
    "    c = keras.layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\", kernel_regularizer=L2)(c)\n",
    "    c = keras.layers.Conv1D(128, 5, padding=\"same\", activation=\"relu\", kernel_regularizer=L2)(c)\n",
    "    gmp, gap = keras.layers.GlobalMaxPooling1D()(c), keras.layers.GlobalAveragePooling1D()(c)\n",
    "    c = keras.layers.Concatenate()([gmp, gap])\n",
    "\n",
    "    # BiGRU with MHA\n",
    "    r = keras.layers.Bidirectional(keras.layers.GRU(gru_units, return_sequences=True, dropout=0.2))(emb)\n",
    "    r = keras.layers.MultiHeadAttention(num_heads=heads, key_dim=embed_dim//heads, dropout=0.1)(r, r)\n",
    "    r = keras.layers.GlobalMaxPooling1D()(r)\n",
    "\n",
    "    aux_h = keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=L2)(aux_in)\n",
    "    aux_h = keras.layers.Dropout(0.2)(aux_h)\n",
    "\n",
    "    h = keras.layers.Concatenate()([c, r, aux_h])\n",
    "    h = keras.layers.Dropout(dropout)(h)\n",
    "    h = keras.layers.Dense(192, activation=\"relu\", kernel_regularizer=L2)(h)\n",
    "    h = keras.layers.Dropout(dropout)(h)\n",
    "    out = keras.layers.Dense(1, activation=\"sigmoid\", name=\"out\")(h)\n",
    "\n",
    "    model = keras.Model([tok_in, aux_in], out, name=\"V2_CNN_BiGRU_MHA\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(2e-3),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[keras.metrics.AUC(name=\"auc\"),\n",
    "                           keras.metrics.Precision(name=\"prec\"),\n",
    "                           keras.metrics.Recall(name=\"rec\")])\n",
    "    return model\n",
    "\n",
    "# ========= 9) Tiny hyperparam sweep driver =========\n",
    "VOCAB_SIZE = MAX_TOKENS + 2\n",
    "N_AUX = feats_tr_all.shape[1]\n",
    "\n",
    "grid = [\n",
    "    # (variant, seq_len, batch, dropout)\n",
    "    (\"V1_StackedBiGRU_Attn\", SEQ_LEN, BATCH, 0.35),\n",
    "    (\"V2_CNN_BiGRU_MHA\",    SEQ_LEN, BATCH, 0.35),\n",
    "]\n",
    "# Optionally expand grid when not in FAST_DEBUG:\n",
    "if not FAST_DEBUG:\n",
    "    grid += [\n",
    "        (\"V1_StackedBiGRU_Attn\", SEQ_LEN, max(16, BATCH//2), 0.35),\n",
    "        (\"V2_CNN_BiGRU_MHA\",    SEQ_LEN, max(16, BATCH//2), 0.30),\n",
    "    ]\n",
    "\n",
    "def train_and_eval(variant, seq_len, batch, dropout):\n",
    "    global SEQ_LEN, BATCH\n",
    "    SEQ_LEN, BATCH = seq_len, batch\n",
    "\n",
    "    # Rebuild datasets with the new SEQ_LEN/BATCH\n",
    "    ds_tr_local = make_ds_with_feats(X_tr_text, feats_tr_split, y_tr, train=True)\n",
    "    ds_va_local = make_ds_with_feats(X_va_text, feats_va_split, y_va, train=False)\n",
    "\n",
    "    # Build model\n",
    "    if variant == \"V1_StackedBiGRU_Attn\":\n",
    "        model = build_v1_stacked_bigru_attn(VOCAB_SIZE, N_AUX, embed_dim=128, gru_units=96, dropout=dropout)\n",
    "    else:\n",
    "        model = build_v2_cnn_bigru_mha(VOCAB_SIZE, N_AUX, embed_dim=128, gru_units=96, dropout=dropout, heads=4)\n",
    "\n",
    "    # Warm-up (compile kernels)\n",
    "    _ = model.predict(ds_tr_local.take(1), verbose=0)\n",
    "\n",
    "    ckpt_dir = (KAGGLE_WORKING if IS_KAGGLE else \"models\")\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    ckpt_path = os.path.join(ckpt_dir, f\"{variant}_best.keras\")\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=5e-5, verbose=1),\n",
    "        keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_loss\", save_best_only=True, verbose=1)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        ds_tr_local, validation_data=ds_va_local, epochs=EPOCHS,\n",
    "        class_weight=CLASS_WEIGHTS, callbacks=callbacks, verbose=1\n",
    "    )\n",
    "\n",
    "    # Tune threshold on validation\n",
    "    va_probs = model.predict(ds_va_local, verbose=0).ravel()\n",
    "    thr_grid = np.linspace(0.30, 0.70, 81)\n",
    "    f1s = [f1_score(y_va, (va_probs >= t).astype(int), average=\"macro\") for t in thr_grid]\n",
    "    best_idx = int(np.argmax(f1s))\n",
    "    best_thr = float(thr_grid[best_idx])\n",
    "    val_f1 = float(f1s[best_idx])\n",
    "\n",
    "    print(f\"[{variant}] SEQ_LEN={seq_len} BATCH={batch} drop={dropout} | Val F1(macro)={val_f1:.4f} @ thr={best_thr:.3f}\")\n",
    "    return model, val_f1, best_thr\n",
    "\n",
    "# Run the sweep and keep the best\n",
    "best = {\"variant\": None, \"f1\": -1.0, \"thr\": 0.5, \"seq_len\": SEQ_LEN, \"batch\": BATCH, \"dropout\": 0.35, \"model\": None}\n",
    "for (v, sl, bs, dr) in grid:\n",
    "    model, f1v, thr = train_and_eval(v, sl, bs, dr)\n",
    "    if f1v > best[\"f1\"]:\n",
    "        best.update({\"variant\": v, \"f1\": f1v, \"thr\": thr, \"seq_len\": sl, \"batch\": bs, \"dropout\": dr, \"model\": model})\n",
    "\n",
    "print(\"\\n=== BEST CONFIG ===\")\n",
    "print(best)\n",
    "\n",
    "# ========= 10) Predict test explicitly (two-input) & build submission =========\n",
    "print(\"Predicting test with best model …\")\n",
    "# Re-tokenise test with current SEQ_LEN\n",
    "import tensorflow as tf\n",
    "\n",
    "# base_vec is the vectorizer you trained/used for this run\n",
    "base_vec = text_vec  # whatever you called it during training\n",
    "\n",
    "def clone_vectorizer_with_length(base_vec, seq_len: int):\n",
    "    # Grab the config of the trained vectorizer\n",
    "    cfg = base_vec.get_config().copy()\n",
    "    # Overwrite the only thing we need to change\n",
    "    cfg[\"output_sequence_length\"] = int(seq_len)\n",
    "\n",
    "    # Build a new vectorizer using the same preprocessing settings\n",
    "    new_vec = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=cfg.get(\"max_tokens\", None),\n",
    "        standardize=cfg.get(\"standardize\", None),\n",
    "        split=cfg.get(\"split\", None),\n",
    "        ngrams=cfg.get(\"ngrams\", None),\n",
    "        output_mode=cfg.get(\"output_mode\", \"int\"),\n",
    "        output_sequence_length=cfg.get(\"output_sequence_length\", None),\n",
    "        pad_to_max_tokens=cfg.get(\"pad_to_max_tokens\", False),\n",
    "        vocabulary=None,  # we’ll set it explicitly just below\n",
    "    )\n",
    "    # Copy the exact learned vocabulary so token IDs match training\n",
    "    new_vec.set_vocabulary(base_vec.get_vocabulary())\n",
    "    return new_vec\n",
    "\n",
    "# Rebuild a vectorizer that matches the best model’s expected seq length\n",
    "text_vec_te = clone_vectorizer_with_length(base_vec, best[\"seq_len\"])\n",
    "\n",
    "# Vectorise the test texts using the cloned vectorizer\n",
    "X_te_tok = text_vec_te(tf.constant(X_test_text))\n",
    "\n",
    "# Predict using the best model\n",
    "test_probs = best[\"model\"].predict([X_te_tok, feats_te], batch_size=best[\"batch\"], verbose=0).ravel()\n",
    "\n",
    "test_pred  = (test_probs >= best[\"thr\"]).astype(int)\n",
    "\n",
    "submission = sample.copy()\n",
    "submission[TARGET_OUT] = test_pred.astype(int)\n",
    "\n",
    "# Validate submission\n",
    "errors = []\n",
    "if list(submission.columns) != list(sample.columns):\n",
    "    errors.append(f\"Columns mismatch. Expected {list(sample.columns)}, got {list(submission.columns)}\")\n",
    "if len(submission) != len(sample):\n",
    "    errors.append(f\"Row count mismatch. Expected {len(sample)}, got {len(submission)}\")\n",
    "if not submission[ID_COL].equals(sample[ID_COL]):\n",
    "    if set(submission[ID_COL]) != set(sample[ID_COL]):\n",
    "        missing = list(sorted(set(sample[ID_COL]) - set(submission[ID_COL])))[:5]\n",
    "        extra   = list(sorted(set(submission[ID_COL]) - set(sample[ID_COL])))[:5]\n",
    "        errors.append(f\"ID set differs. Missing: {missing} | Extra: {extra}\")\n",
    "    else:\n",
    "        errors.append(\"ID order differs from sample. Must match sample_submission order.\")\n",
    "if submission[TARGET_OUT].isna().any():\n",
    "    errors.append(\"Target has NaNs.\")\n",
    "u = set(np.unique(submission[TARGET_OUT]))\n",
    "if not u.issubset({0,1}):\n",
    "    errors.append(f\"Target invalid values {sorted(u)}; must be 0/1.\")\n",
    "if errors:\n",
    "    print(\"❌ Submission invalid:\"); [print(\" -\", e) for e in errors]; raise SystemExit(1)\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    submission.to_csv(OUT_KAGGLE, index=False)\n",
    "    print(f\"✅ Saved Kaggle file: {OUT_KAGGLE}\")\n",
    "submission.to_csv(OUT_LOCAL, index=False)\n",
    "print(f\"✅ Saved local copy : {OUT_LOCAL}\")\n",
    "\n",
    "# ========= 11) Log run info =========\n",
    "run_info = {\n",
    "    \"task\": \"3.2_neural_advanced\",\n",
    "    \"best\": {\n",
    "        \"variant\": best[\"variant\"],\n",
    "        \"val_f1_macro\": float(best[\"f1\"]),\n",
    "        \"threshold\": float(best[\"thr\"]),\n",
    "        \"seq_len\": int(best[\"seq_len\"]),\n",
    "        \"batch\": int(best[\"batch\"]),\n",
    "        \"dropout\": float(best[\"dropout\"]),\n",
    "    },\n",
    "    \"seed\": SEED,\n",
    "    \"time\": datetime.now().isoformat(timespec=\"seconds\")\n",
    "}\n",
    "with open(\"results/run_06_neural_advanced.json\",\"w\") as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "\n",
    "print(\"\\nModel used:\", best[\"variant\"])\n",
    "print(f\"Validation F1 (macro): {best['f1']:.4f} at threshold {best['thr']:.3f}\")\n",
    "print(\"Final submission head:\\n\", submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eef7ba-218c-43f9-ac99-ffde595e4650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac-py312)",
   "language": "python",
   "name": "tf-mac-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
