{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049581a1-38f2-4ff0-beb3-70f00b39d2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\n",
      "NumPy : 1.26.4\n",
      "Pandas: 2.2.3\n",
      "📄 Loading train.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/train.csv\n",
      "📄 Loading test.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/test.csv\n",
      "📄 Loading sample_submission.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/sample_submission.csv\n",
      "Train shape: (2029, 9)\n",
      "Test  shape: (10, 8)\n",
      "Sample shape: (10, 2)\n",
      "TEXT_COL  = body\n",
      "TARGET_COL= rule_violation\n",
      "ID_COL    = row_id\n",
      "✅ Train features: (2029, 42)\n",
      "✅ Test  features: (10, 42)\n",
      "\n",
      "Top 15 features by mutual information:\n",
      "              feature        mi\n",
      "4        avg_word_len  0.067606\n",
      "21           md_links  0.044872\n",
      "0          char_count  0.041142\n",
      "6          caps_ratio  0.040016\n",
      "40         excl_ratio  0.036055\n",
      "41         ques_ratio  0.035856\n",
      "15        punct_ratio  0.033731\n",
      "2     uniq_word_count  0.030241\n",
      "36       negate_count  0.025696\n",
      "33          you_ratio  0.023116\n",
      "25          num_ratio  0.021872\n",
      "1          word_count  0.021510\n",
      "34            i_ratio  0.021109\n",
      "31          you_count  0.019515\n",
      "3   lexical_diversity  0.015952\n",
      "\n",
      "Saved:\n",
      " - data/processed/train_features.pkl\n",
      " - data/processed/test_features.pkl\n",
      " - results/feature_mi_rank.csv\n",
      " - results/FEATURES_SUMMARY.txt\n"
     ]
    }
   ],
   "source": [
    "# 03_feature_engineering.ipynb — Jigsaw Agile Community Rules (Local + Kaggle safe)\n",
    "# - Auto-finds data files locally or in Kaggle\n",
    "# - Builds lightweight features (no external deps)\n",
    "# - Persists features + MI ranking for Week 2 notes\n",
    "\n",
    "# ===== 0) Imports & env info =====\n",
    "import sys, os, glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"NumPy :\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "\n",
    "# Where to save outputs\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# ===== 1) Data file discovery (NO need to restructure) =====\n",
    "# Optional: if you know exactly where your CSVs live locally, set this:\n",
    "DATA_ROOT = \"\"  # e.g. r\"/Users/michael/projects/jigsaw-competition/data/raw\"  (leave empty to auto-detect)\n",
    "\n",
    "KAGGLE_DIR = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "CANDIDATE_DIRS = [\n",
    "    \".\", \"..\", \"../..\", \"../../..\",\n",
    "    \"data/raw\", \"../data/raw\", \"../../data/raw\",\n",
    "    \"jigsaw-agile-community-rules\", \"../jigsaw-agile-community-rules\"\n",
    "]\n",
    "\n",
    "if DATA_ROOT:\n",
    "    # Prioritise your explicit local path\n",
    "    CANDIDATE_DIRS.insert(0, DATA_ROOT)\n",
    "\n",
    "def _candidate_paths(filename: str):\n",
    "    paths = []\n",
    "    # Kaggle location\n",
    "    if os.path.exists(KAGGLE_DIR):\n",
    "        paths.append(os.path.join(KAGGLE_DIR, filename))\n",
    "    # Common local locations\n",
    "    for d in CANDIDATE_DIRS:\n",
    "        paths.append(os.path.join(d, filename))\n",
    "    # Recursive glob (last resort)\n",
    "    paths.extend(glob.glob(f\"**/{filename}\", recursive=True))\n",
    "    # Deduplicate while preserving order, include only existing\n",
    "    seen, out = set(), []\n",
    "    for p in paths:\n",
    "        ap = os.path.abspath(p)\n",
    "        if ap not in seen and os.path.exists(ap):\n",
    "            seen.add(ap); out.append(ap)\n",
    "    return out\n",
    "\n",
    "def read_first_csv(filename: str):\n",
    "    found = _candidate_paths(filename)\n",
    "    if not found:\n",
    "        print(f\"\\n❌ Could not find '{filename}'.\")\n",
    "        print(\"Searched relative to:\", os.getcwd())\n",
    "        print(\"Tried these dirs:\\n\" + \"\\n\".join(\" - \" + os.path.abspath(d) for d in CANDIDATE_DIRS))\n",
    "        print(\"\\nFix options:\")\n",
    "        print(\"  1) Put train.csv/test.csv/sample_submission.csv under 'data/raw/'\")\n",
    "        print(\"  2) Or set DATA_ROOT above to the folder containing them\")\n",
    "        raise FileNotFoundError(filename)\n",
    "    print(f\"📄 Loading {filename} from: {found[0]}\")\n",
    "    return pd.read_csv(found[0])\n",
    "\n",
    "train_df = read_first_csv(\"train.csv\")\n",
    "test_df  = read_first_csv(\"test.csv\")\n",
    "sample   = read_first_csv(\"sample_submission.csv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test  shape:\", test_df.shape)\n",
    "print(\"Sample shape:\", sample.shape)\n",
    "\n",
    "# ===== 2) Robust column detection =====\n",
    "TEXT_COL = next((c for c in [\"comment_text\", \"body\", \"text\"] if c in train_df.columns), None)\n",
    "TARGET_COL = next((c for c in [\"rule_violation\", \"target\", \"label\"] if c in train_df.columns), None)\n",
    "ID_COL = next((c for c in [\"row_id\", \"id\", \"ID\"] if c in sample.columns), None)\n",
    "\n",
    "if TEXT_COL is None:\n",
    "    raise ValueError(\"Couldn't find a text column in train_df (expected one of: comment_text/body/text).\")\n",
    "if TARGET_COL is None:\n",
    "    raise ValueError(\"Couldn't find a target column in train_df (expected one of: rule_violation/target/label).\")\n",
    "if ID_COL is None:\n",
    "    raise ValueError(\"Couldn't find an ID column in sample_submission (expected row_id/id/ID).\")\n",
    "\n",
    "print(f\"TEXT_COL  = {TEXT_COL}\")\n",
    "print(f\"TARGET_COL= {TARGET_COL}\")\n",
    "print(f\"ID_COL    = {ID_COL}\")\n",
    "\n",
    "# ===== 3) Lightweight, dependency-free feature extractor =====\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "NEG_WORDS = {\n",
    "    \"ban\",\"banned\",\"remove\",\"removed\",\"delete\",\"deleted\",\"violation\",\"warn\",\"warning\",\n",
    "    \"report\",\"flag\",\"hate\",\"toxic\",\"idiot\",\"stupid\",\"dumb\",\"trash\",\"nonsense\",\"shut\",\"shutup\",\n",
    "    \"racist\",\"sexist\",\"harass\",\"abuse\",\"spam\",\"brigade\",\"rule\",\"rules\",\"automod\",\"mod\",\"moderator\"\n",
    "}\n",
    "POS_WORDS = {\"please\",\"thanks\",\"thank\",\"appreciate\",\"sorry\",\"kindly\",\"cheers\"}\n",
    "QUESTION_WORDS = {\"why\",\"how\",\"what\",\"when\",\"where\",\"which\",\"who\"}\n",
    "NEGATIONS = {\"not\",\"no\",\"never\",\"n't\"}\n",
    "\n",
    "EMOJI_RE = re.compile(r\"[\\U0001F300-\\U0001FAFF]\")\n",
    "REPEAT_CHAR_RE = re.compile(r\"(.)\\1{2,}\")         # loooool, ???!!!\n",
    "MD_LINK_RE = re.compile(r\"\\[[^\\]]+\\]\\([^)]+\\)\")   # [text](url)\n",
    "\n",
    "def _safe_div(num, den):\n",
    "    den = np.maximum(den, 1)\n",
    "    return num / den\n",
    "\n",
    "def _count_tokens(text, vocab):\n",
    "    toks = text.lower().split()\n",
    "    return sum(t in vocab for t in toks)\n",
    "\n",
    "def extract_features_v2(df, text_col):\n",
    "    s = df[text_col].fillna(\"\").astype(str)\n",
    "    feats = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Basic length / density\n",
    "    feats[\"char_count\"]  = s.str.len()\n",
    "    feats[\"word_count\"]  = s.str.split().str.len().astype(\"int64\")\n",
    "    feats[\"uniq_word_count\"] = s.apply(lambda x: len(set(x.lower().split())))\n",
    "    feats[\"lexical_diversity\"] = _safe_div(feats[\"uniq_word_count\"], feats[\"word_count\"])\n",
    "    feats[\"avg_word_len\"] = _safe_div(feats[\"char_count\"], feats[\"word_count\"])\n",
    "\n",
    "    # Casing\n",
    "    feats[\"upper_count\"] = s.str.count(r\"[A-Z]\")\n",
    "    feats[\"caps_ratio\"]  = _safe_div(feats[\"upper_count\"], feats[\"char_count\"])\n",
    "    feats[\"all_caps_words\"] = s.str.count(r\"\\b[A-Z]{2,}\\b\")\n",
    "\n",
    "    # Punctuation / structure\n",
    "    feats[\"excl_count\"]  = s.str.count(\"!\")\n",
    "    feats[\"ques_count\"]  = s.str.count(r\"\\?\")\n",
    "    feats[\"dots_count\"]  = s.str.count(r\"\\.\")\n",
    "    feats[\"ellipsis_count\"] = s.str.count(r\"\\.\\.\\.\")\n",
    "    feats[\"multi_excl\"]  = s.str.count(r\"!!+\")\n",
    "    feats[\"multi_ques\"]  = s.str.count(r\"\\?\\?+\")\n",
    "    feats[\"mix_punct\"]   = s.str.count(r\"[!?]{2,}\")\n",
    "    feats[\"punct_ratio\"] = _safe_div(feats[\"excl_count\"] + feats[\"ques_count\"] + feats[\"dots_count\"], feats[\"char_count\"])\n",
    "\n",
    "    # Repeats / elongations\n",
    "    feats[\"repeat_char\"] = s.apply(lambda x: len(REPEAT_CHAR_RE.findall(x)))\n",
    "\n",
    "    # Reddit / markdown cues\n",
    "    feats[\"has_user_mention\"]      = s.str.contains(r\"u/\\w+\", case=False, regex=True).astype(\"int8\")\n",
    "    feats[\"has_subreddit_mention\"] = s.str.contains(r\"r/\\w+\", case=False, regex=True).astype(\"int8\")\n",
    "    feats[\"quote_count\"]           = s.str.count(r\"^>|\\n>\", flags=re.MULTILINE)\n",
    "    feats[\"code_ticks\"]            = s.str.count(r\"`\")\n",
    "    feats[\"md_links\"]              = s.apply(lambda x: len(MD_LINK_RE.findall(x))).astype(\"int16\")\n",
    "\n",
    "    # URLs / emails / numbers\n",
    "    feats[\"has_url\"]     = s.str.contains(r\"http[s]?://\", case=False, regex=True).astype(\"int8\")\n",
    "    feats[\"email_count\"] = s.str.count(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "    feats[\"digit_count\"] = s.str.count(r\"\\d\")\n",
    "    feats[\"num_ratio\"]   = _safe_div(feats[\"digit_count\"], feats[\"char_count\"])\n",
    "\n",
    "    # Emoji\n",
    "    feats[\"emoji_count\"] = s.apply(lambda x: len(EMOJI_RE.findall(x))).astype(\"int16\")\n",
    "\n",
    "    # Lexicon cues\n",
    "    feats[\"neg_lex_count\"] = s.apply(lambda x: _count_tokens(x, NEG_WORDS)).astype(\"int16\")\n",
    "    feats[\"pos_lex_count\"] = s.apply(lambda x: _count_tokens(x, POS_WORDS)).astype(\"int16\")\n",
    "    feats[\"neg_lex_ratio\"] = _safe_div(feats[\"neg_lex_count\"], feats[\"word_count\"])\n",
    "    feats[\"pos_lex_ratio\"] = _safe_div(feats[\"pos_lex_count\"], feats[\"word_count\"])\n",
    "\n",
    "    # Pronouns / stance\n",
    "    feats[\"you_count\"] = s.str.count(r\"\\byou\\b\", flags=re.IGNORECASE)\n",
    "    feats[\"i_count\"]   = s.str.count(r\"\\bi\\b\",   flags=re.IGNORECASE)\n",
    "    feats[\"you_ratio\"] = _safe_div(feats[\"you_count\"], feats[\"word_count\"])\n",
    "    feats[\"i_ratio\"]   = _safe_div(feats[\"i_count\"],   feats[\"word_count\"])\n",
    "\n",
    "    # Questions & negations\n",
    "    feats[\"wh_q_count\"]  = s.apply(lambda x: _count_tokens(x, QUESTION_WORDS)).astype(\"int16\")\n",
    "    feats[\"negate_count\"]= s.apply(lambda x: _count_tokens(x, NEGATIONS)).astype(\"int16\")\n",
    "\n",
    "    # Start/End cues\n",
    "    feats[\"starts_with_quote\"] = s.str.match(r'^\\s*[\"\\']').astype(\"int8\")\n",
    "    feats[\"ends_with_q\"]       = s.str.endswith(\"?\").astype(\"int8\")\n",
    "    feats[\"ends_with_excl\"]    = s.str.endswith(\"!\").astype(\"int8\")\n",
    "\n",
    "    # Ratios\n",
    "    feats[\"excl_ratio\"] = _safe_div(feats[\"excl_count\"], feats[\"char_count\"])\n",
    "    feats[\"ques_ratio\"] = _safe_div(feats[\"ques_count\"], feats[\"char_count\"])\n",
    "\n",
    "    # Cleanup\n",
    "    feats = feats.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    return feats\n",
    "\n",
    "# ===== 4) Extract features and persist =====\n",
    "train_features = extract_features_v2(train_df, TEXT_COL)\n",
    "test_features  = extract_features_v2(test_df,  TEXT_COL)\n",
    "\n",
    "train_features.to_pickle(\"data/processed/train_features.pkl\")\n",
    "test_features.to_pickle(\"data/processed/test_features.pkl\")\n",
    "\n",
    "print(\"✅ Train features:\", train_features.shape)\n",
    "print(\"✅ Test  features:\", test_features.shape)\n",
    "\n",
    "# ===== 5) Mutual Information ranking =====\n",
    "y = train_df[TARGET_COL].astype(int).values\n",
    "mi = mutual_info_classif(train_features.values, y, discrete_features=False, random_state=42)\n",
    "mi_rank = pd.DataFrame({\"feature\": train_features.columns, \"mi\": mi}).sort_values(\"mi\", ascending=False)\n",
    "mi_rank.to_csv(\"results/feature_mi_rank.csv\", index=False)\n",
    "\n",
    "print(\"\\nTop 15 features by mutual information:\")\n",
    "print(mi_rank.head(15))\n",
    "\n",
    "# Optional: summary file for your project doc\n",
    "with open(\"results/FEATURES_SUMMARY.txt\", \"w\") as f:\n",
    "    f.write(f\"TEXT_COL={TEXT_COL}\\nTARGET_COL={TARGET_COL}\\nID_COL={ID_COL}\\n\")\n",
    "    f.write(f\"n_train_features={train_features.shape[1]}\\n\")\n",
    "    f.write(\"TOP_15_MI_FEATURES:\\n\")\n",
    "    for row in mi_rank.head(15).itertuples(index=False):\n",
    "        f.write(f\"- {row.feature}: {row.mi:.6f}\\n\")\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - data/processed/train_features.pkl\")\n",
    "print(\" - data/processed/test_features.pkl\")\n",
    "print(\" - results/feature_mi_rank.csv\")\n",
    "print(\" - results/FEATURES_SUMMARY.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0993e44-884f-4c6c-80aa-a371af74f132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training quick baseline (TF-IDF + LR; scaling engineered features only) to produce submission.csv ...\n",
      "✅ Saved local copy : submissions/submission.csv\n",
      "Final submission shape/cols: (10, 2) ['row_id', 'rule_violation']\n",
      "Target dtype/unique: int64 [0, 1]\n",
      "Head:\n",
      "    row_id  rule_violation\n",
      "0    2029               0\n",
      "1    2030               0\n",
      "2    2031               0\n",
      "3    2032               1\n",
      "4    2033               1\n"
     ]
    }
   ],
   "source": [
    "# ===== 6) Build and save Kaggle-ready submission.csv (works local + Kaggle) =====\n",
    "import os, glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 6.1 — Ensure we know where to save\n",
    "KAGGLE_WORKING = \"/kaggle/working\"\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "OUT_KAGGLE = os.path.join(KAGGLE_WORKING, \"submission.csv\") if IS_KAGGLE else None\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "OUT_LOCAL = \"submissions/submission.csv\"\n",
    "\n",
    "# We expect these from earlier cells; but re-detect defensively if needed\n",
    "if \"sample\" not in globals():\n",
    "    # try to load sample again\n",
    "    def _find_one(name):\n",
    "        cand = []\n",
    "        if os.path.exists(\"/kaggle/input/jigsaw-agile-community-rules\"):\n",
    "            cand.append(f\"/kaggle/input/jigsaw-agile-community-rules/{name}\")\n",
    "        cand += [name, f\"data/raw/{name}\"]\n",
    "        for p in cand + glob.glob(f\"**/{name}\", recursive=True):\n",
    "            if os.path.exists(p):\n",
    "                return pd.read_csv(p)\n",
    "        raise FileNotFoundError(name)\n",
    "    sample = _find_one(\"sample_submission.csv\")\n",
    "\n",
    "ID_COL, TARGET_OUT = sample.columns.tolist()\n",
    "\n",
    "if \"TEXT_COL\" not in globals():\n",
    "    TEXT_COL = next((c for c in [\"comment_text\",\"body\",\"text\"] if c in train_df.columns), None)\n",
    "if \"TARGET_COL\" not in globals():\n",
    "    TARGET_COL = next((c for c in [\"rule_violation\",\"target\",\"label\"] if c in train_df.columns), None)\n",
    "assert TEXT_COL is not None and TARGET_COL is not None, \"Need TEXT_COL and TARGET_COL.\"\n",
    "\n",
    "# 6.2 — Try to use an uploaded predictions CSV (fast path on Kaggle)\n",
    "preds_df = None\n",
    "preds_paths_priority = [\n",
    "    \"/kaggle/input/submission-v1-baseline/submission_v1_baseline.csv\",  # your known upload\n",
    "]\n",
    "# generic search under /kaggle/input for any submission-like csv (excluding sample_submission)\n",
    "if IS_KAGGLE:\n",
    "    for root, _, files in os.walk(\"/kaggle/input\"):\n",
    "        for fn in files:\n",
    "            f = os.path.join(root, fn)\n",
    "            if fn.lower().endswith(\".csv\") and \"sample_submission\" not in fn.lower():\n",
    "                if \"submission\" in fn.lower() or \"pred\" in fn.lower():\n",
    "                    preds_paths_priority.append(f)\n",
    "\n",
    "def _load_first_existing(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                df = pd.read_csv(p)\n",
    "                print(f\"Using predictions from: {p} | shape={df.shape} | cols={list(df.columns)}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"Skip {p} ({e})\")\n",
    "    return None\n",
    "\n",
    "preds_df = _load_first_existing(preds_paths_priority)\n",
    "\n",
    "def _build_submission_from_preds(sample_df, preds_df):\n",
    "    \"\"\"Map arbitrary preds file into sample schema robustly.\"\"\"\n",
    "    id_col_candidates = [ID_COL, \"row_id\",\"id\",\"ID\",\"Row_ID\"]\n",
    "    target_col_candidates = [TARGET_OUT,\"rule_violation\",\"target\",\"prediction\",\"pred\",\"prob\",\"label\"]\n",
    "    id_in = next((c for c in id_col_candidates if c in preds_df.columns), None)\n",
    "    tgt_in = next((c for c in target_col_candidates if c in preds_df.columns), None)\n",
    "\n",
    "    if id_in is not None and tgt_in is not None:\n",
    "        # Dedup by ID (take last)\n",
    "        if preds_df[id_in].duplicated().any():\n",
    "            preds_base = preds_df.drop_duplicates(subset=[id_in], keep=\"last\").copy()\n",
    "        else:\n",
    "            preds_base = preds_df.copy()\n",
    "        tmp = preds_base[[id_in, tgt_in]].rename(columns={id_in: ID_COL, tgt_in: TARGET_OUT})\n",
    "        sub = sample_df[[ID_COL]].merge(tmp, on=ID_COL, how=\"left\")\n",
    "    else:\n",
    "        # No ID in preds: fallback to by-order (must match length)\n",
    "        assert len(preds_df) == len(sample_df), \"Preds length must match sample_submission for order-based mapping.\"\n",
    "        # choose first numeric/boolean-like column\n",
    "        use_col = None\n",
    "        for c in preds_df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(preds_df[c]) or preds_df[c].dtype == bool:\n",
    "                use_col = c; break\n",
    "        if use_col is None:\n",
    "            use_col = preds_df.columns[0]\n",
    "        sub = sample_df.copy()\n",
    "        sub[TARGET_OUT] = preds_df[use_col].values\n",
    "\n",
    "    # Coerce to 0/1 ints\n",
    "    vals = pd.to_numeric(sub[TARGET_OUT], errors=\"coerce\").fillna(0)\n",
    "    if not np.array_equal(np.unique(vals), [0,1]):\n",
    "        if vals.min() >= 0 and vals.max() <= 1:\n",
    "            vals = (vals >= 0.5).astype(int)\n",
    "        else:\n",
    "            vals = vals.round().clip(0,1).astype(int)\n",
    "    sub[TARGET_OUT] = vals\n",
    "    return sub\n",
    "\n",
    "submission = None\n",
    "if preds_df is not None:\n",
    "    try:\n",
    "        submission = _build_submission_from_preds(sample, preds_df)\n",
    "        print(\"Built submission from uploaded predictions.\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to build from uploaded predictions, will train a quick baseline instead:\", e)\n",
    "\n",
    "# 6.3 — If no preds available, train a tiny baseline (TF-IDF + LR, scale only engineered features) and predict\n",
    "print(\"Training quick baseline (TF-IDF + LR; scaling engineered features only) to produce submission.csv ...\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "# Ensure features exist (if this block is run standalone)\n",
    "if \"train_features\" not in globals() or \"test_features\" not in globals():\n",
    "    train_features = extract_features_v2(train_df, TEXT_COL)\n",
    "    test_features  = extract_features_v2(test_df,  TEXT_COL)\n",
    "\n",
    "# 1) TF-IDF on text (sparse)\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=3, max_df=0.9)\n",
    "X_text      = tfidf.fit_transform(train_df[TEXT_COL].fillna(\"\").astype(str))     # sparse CSR\n",
    "X_text_test = tfidf.transform( test_df[TEXT_COL].fillna(\"\").astype(str))\n",
    "\n",
    "# 2) Scale ONLY the engineered features (they're small & dense)\n",
    "feats_train = train_features.values.astype(np.float32)\n",
    "feats_test  = test_features.values.astype(np.float32)\n",
    "\n",
    "scaler = StandardScaler()      # dense scaler is fine here (42 cols)\n",
    "feats_train_scaled = scaler.fit_transform(feats_train)\n",
    "feats_test_scaled  = scaler.transform(feats_test)\n",
    "\n",
    "# 3) Convert scaled features to sparse and hstack with TF-IDF\n",
    "X_feat      = sp.csr_matrix(feats_train_scaled)\n",
    "X_feat_test = sp.csr_matrix(feats_test_scaled)\n",
    "\n",
    "X      = sp.hstack([X_text, X_feat], format=\"csr\")   # ensure CSR\n",
    "X_test = sp.hstack([X_text_test, X_feat_test], format=\"csr\")\n",
    "\n",
    "y = train_df[TARGET_COL].astype(int).values\n",
    "\n",
    "# 4) Logistic Regression tuned for sparse, high-dim data\n",
    "lr = LogisticRegression(\n",
    "    solver=\"saga\",            # robust for large sparse design matrices\n",
    "    penalty=\"l2\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=3000,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lr.fit(X, y)\n",
    "preds = lr.predict(X_test).astype(int)\n",
    "\n",
    "# 5) Build submission in sample order/IDs\n",
    "submission = sample.copy()\n",
    "submission[TARGET_OUT] = preds\n",
    "\n",
    "\n",
    "# 6.4 — Validate strictly\n",
    "errors = []\n",
    "if list(submission.columns) != list(sample.columns):\n",
    "    errors.append(f\"Columns mismatch. Expected {list(sample.columns)}, got {list(submission.columns)}\")\n",
    "if len(submission) != len(sample):\n",
    "    errors.append(f\"Row count mismatch. Expected {len(sample)}, got {len(submission)}\")\n",
    "if not submission[ID_COL].equals(sample[ID_COL]):\n",
    "    if set(submission[ID_COL]) != set(sample[ID_COL]):\n",
    "        missing = list(sorted(set(sample[ID_COL]) - set(submission[ID_COL])))[:5]\n",
    "        extra   = list(sorted(set(submission[ID_COL]) - set(sample[ID_COL])))[:5]\n",
    "        errors.append(f\"ID set differs. Missing: {missing} | Extra: {extra}\")\n",
    "    else:\n",
    "        errors.append(\"ID order differs from sample. Must match sample_submission order.\")\n",
    "if submission[TARGET_OUT].isna().any():\n",
    "    errors.append(\"Target has NaNs.\")\n",
    "u = set(np.unique(submission[TARGET_OUT]))\n",
    "if not u.issubset({0,1}):\n",
    "    errors.append(f\"Target has invalid values {sorted(u)}; must be 0/1.\")\n",
    "\n",
    "if errors:\n",
    "    print(\"❌ Submission invalid:\")\n",
    "    for e in errors: print(\" -\", e)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# 6.5 — Save to Kaggle working (if present) and local\n",
    "if IS_KAGGLE:\n",
    "    submission.to_csv(OUT_KAGGLE, index=False)\n",
    "    print(f\"✅ Saved Kaggle file: {OUT_KAGGLE}\")\n",
    "if OUT_LOCAL:\n",
    "    submission.to_csv(OUT_LOCAL, index=False)\n",
    "    print(f\"✅ Saved local copy : {OUT_LOCAL}\")\n",
    "\n",
    "# 6.6 — Final sanity print\n",
    "print(\"Final submission shape/cols:\", submission.shape, list(submission.columns))\n",
    "print(\"Target dtype/unique:\", submission[TARGET_OUT].dtype, sorted(submission[TARGET_OUT].unique()))\n",
    "print(\"Head:\\n\", submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5c48a-ff6f-4393-b016-3baf90cbb0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
