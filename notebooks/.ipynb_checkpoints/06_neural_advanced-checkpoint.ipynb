{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33274bca-1e97-46c6-8aeb-f60cfbda414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 08:03:38) [Clang 14.0.6 ]\n",
      "NumPy : 1.26.4\n",
      "Pandas: 2.2.3\n",
      "TensorFlow: 2.16.2\n",
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "📄 Loading train.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/train.csv\n",
      "📄 Loading test.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/test.csv\n",
      "📄 Loading sample_submission.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/sample_submission.csv\n",
      "Train shape: (2029, 9)\n",
      "Test  shape: (10, 8)\n",
      "Sample shape: (10, 2)\n",
      "TEXT_COL  = body\n",
      "TARGET_COL= rule_violation\n",
      "ID_COL    = row_id | TARGET_OUT = rule_violation\n",
      "Class weights: {0: 1.0169172932330828, 1: 0.9836363636363636}\n",
      "✅ Loaded engineered features from 03.\n",
      "Features ready. Train: (2029, 42) | Test: (10, 42)\n",
      "[FAST_DEBUG=False] HAS_GPU=True | SEQ_LEN=200 | BATCH=64 | EPOCHS=14\n",
      "Sample batch: (64, 200) (64, 42) (64,)\n",
      "Epoch 1/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - auc: 0.5837 - loss: 0.7994 - prec: 0.5718 - rec: 0.5856\n",
      "Epoch 1: val_loss improved from None to 0.62727, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 416ms/step - auc: 0.5901 - loss: 0.7799 - prec: 0.5725 - rec: 0.5745 - val_auc: 0.7064 - val_loss: 0.6273 - val_prec: 0.6683 - val_rec: 0.6553 - learning_rate: 0.0020\n",
      "Epoch 2/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - auc: 0.6269 - loss: 0.7131 - prec: 0.5780 - rec: 0.5897\n",
      "Epoch 2: val_loss did not improve from 0.62727\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 348ms/step - auc: 0.6318 - loss: 0.7106 - prec: 0.5954 - rec: 0.5903 - val_auc: 0.6859 - val_loss: 0.6426 - val_prec: 0.6109 - val_rec: 0.8155 - learning_rate: 0.0020\n",
      "Epoch 3/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - auc: 0.6350 - loss: 0.6971 - prec: 0.6091 - rec: 0.6794\n",
      "Epoch 3: val_loss improved from 0.62727 to 0.61506, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 387ms/step - auc: 0.6368 - loss: 0.7104 - prec: 0.6091 - rec: 0.6194 - val_auc: 0.7065 - val_loss: 0.6151 - val_prec: 0.6163 - val_rec: 0.7330 - learning_rate: 0.0020\n",
      "Epoch 4/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - auc: 0.6316 - loss: 0.7246 - prec: 0.6028 - rec: 0.6626\n",
      "Epoch 4: val_loss did not improve from 0.61506\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 348ms/step - auc: 0.6571 - loss: 0.6955 - prec: 0.6246 - rec: 0.6655 - val_auc: 0.7027 - val_loss: 0.6288 - val_prec: 0.5993 - val_rec: 0.7913 - learning_rate: 0.0020\n",
      "Epoch 5/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - auc: 0.6532 - loss: 0.6956 - prec: 0.6028 - rec: 0.6588\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.61506\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 345ms/step - auc: 0.6484 - loss: 0.6923 - prec: 0.5971 - rec: 0.6000 - val_auc: 0.7000 - val_loss: 0.6206 - val_prec: 0.6100 - val_rec: 0.7670 - learning_rate: 0.0020\n",
      "Epoch 6/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - auc: 0.6459 - loss: 0.7036 - prec: 0.6131 - rec: 0.6767\n",
      "Epoch 6: val_loss improved from 0.61506 to 0.60547, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 349ms/step - auc: 0.6533 - loss: 0.6939 - prec: 0.6090 - rec: 0.6533 - val_auc: 0.7418 - val_loss: 0.6055 - val_prec: 0.7104 - val_rec: 0.6311 - learning_rate: 0.0010\n",
      "Epoch 7/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - auc: 0.7335 - loss: 0.6130 - prec: 0.6699 - rec: 0.6652\n",
      "Epoch 7: val_loss improved from 0.60547 to 0.54297, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 354ms/step - auc: 0.7394 - loss: 0.6134 - prec: 0.6766 - rec: 0.6618 - val_auc: 0.8042 - val_loss: 0.5430 - val_prec: 0.7182 - val_rec: 0.7670 - learning_rate: 0.0010\n",
      "Epoch 8/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - auc: 0.8821 - loss: 0.4529 - prec: 0.7956 - rec: 0.8236\n",
      "Epoch 8: val_loss did not improve from 0.54297\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 360ms/step - auc: 0.8980 - loss: 0.4183 - prec: 0.8307 - rec: 0.8206 - val_auc: 0.8203 - val_loss: 0.6536 - val_prec: 0.7261 - val_rec: 0.8107 - learning_rate: 0.0010\n",
      "Epoch 9/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - auc: 0.9616 - loss: 0.2529 - prec: 0.9293 - rec: 0.9181\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.54297\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 351ms/step - auc: 0.9500 - loss: 0.2921 - prec: 0.9127 - rec: 0.9248 - val_auc: 0.8295 - val_loss: 0.5961 - val_prec: 0.6691 - val_rec: 0.8932 - learning_rate: 0.0010\n",
      "Epoch 10/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - auc: 0.9596 - loss: 0.2520 - prec: 0.8864 - rec: 0.8907\n",
      "Epoch 10: val_loss did not improve from 0.54297\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 348ms/step - auc: 0.9682 - loss: 0.2274 - prec: 0.9013 - rec: 0.9188 - val_auc: 0.8161 - val_loss: 0.8728 - val_prec: 0.6691 - val_rec: 0.8738 - learning_rate: 5.0000e-04\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "[V1_StackedBiGRU_Attn] SEQ_LEN=200 BATCH=64 drop=0.35 | Val F1(macro)=0.7291 @ thr=0.555\n",
      "Epoch 1/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - auc: 0.5426 - loss: 0.8022 - prec: 0.5455 - rec: 0.5310\n",
      "Epoch 1: val_loss improved from None to 0.61589, saving model to models/V2_CNN_BiGRU_MHA_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 352ms/step - auc: 0.6029 - loss: 0.7560 - prec: 0.5843 - rec: 0.5758 - val_auc: 0.7401 - val_loss: 0.6159 - val_prec: 0.7016 - val_rec: 0.6505 - learning_rate: 0.0020\n",
      "Epoch 2/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - auc: 0.6765 - loss: 0.6862 - prec: 0.6129 - rec: 0.6417\n",
      "Epoch 2: val_loss improved from 0.61589 to 0.53082, saving model to models/V2_CNN_BiGRU_MHA_best.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 283ms/step - auc: 0.7087 - loss: 0.6478 - prec: 0.6444 - rec: 0.6897 - val_auc: 0.8260 - val_loss: 0.5308 - val_prec: 0.7333 - val_rec: 0.8010 - learning_rate: 0.0020\n",
      "Epoch 3/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - auc: 0.8792 - loss: 0.4612 - prec: 0.7949 - rec: 0.8354\n",
      "Epoch 3: val_loss did not improve from 0.53082\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 279ms/step - auc: 0.8847 - loss: 0.4561 - prec: 0.8070 - rec: 0.8061 - val_auc: 0.8440 - val_loss: 0.5344 - val_prec: 0.8132 - val_rec: 0.7184 - learning_rate: 0.0020\n",
      "Epoch 4/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - auc: 0.9638 - loss: 0.2566 - prec: 0.9125 - rec: 0.8960\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.53082\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 275ms/step - auc: 0.9689 - loss: 0.2405 - prec: 0.9220 - rec: 0.9176 - val_auc: 0.8283 - val_loss: 0.7447 - val_prec: 0.7824 - val_rec: 0.7330 - learning_rate: 0.0020\n",
      "Epoch 5/14\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - auc: 0.9969 - loss: 0.0813 - prec: 0.9798 - rec: 0.9813\n",
      "Epoch 5: val_loss did not improve from 0.53082\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 276ms/step - auc: 0.9941 - loss: 0.1002 - prec: 0.9732 - rec: 0.9697 - val_auc: 0.8243 - val_loss: 0.8426 - val_prec: 0.7778 - val_rec: 0.7476 - learning_rate: 0.0010\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "[V2_CNN_BiGRU_MHA] SEQ_LEN=200 BATCH=64 drop=0.35 | Val F1(macro)=0.7683 @ thr=0.630\n",
      "Epoch 1/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - auc: 0.5394 - loss: 0.8521 - prec: 0.5447 - rec: 0.5251\n",
      "Epoch 1: val_loss improved from None to 0.63725, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 267ms/step - auc: 0.5914 - loss: 0.7731 - prec: 0.5733 - rec: 0.5685 - val_auc: 0.6940 - val_loss: 0.6373 - val_prec: 0.6287 - val_rec: 0.7233 - learning_rate: 0.0020\n",
      "Epoch 2/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - auc: 0.6081 - loss: 0.7345 - prec: 0.5679 - rec: 0.5741\n",
      "Epoch 2: val_loss improved from 0.63725 to 0.62726, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 250ms/step - auc: 0.6225 - loss: 0.7205 - prec: 0.5909 - rec: 0.5988 - val_auc: 0.7073 - val_loss: 0.6273 - val_prec: 0.6522 - val_rec: 0.7282 - learning_rate: 0.0020\n",
      "Epoch 3/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - auc: 0.6227 - loss: 0.7003 - prec: 0.5770 - rec: 0.6025\n",
      "Epoch 3: val_loss improved from 0.62726 to 0.62326, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 246ms/step - auc: 0.6073 - loss: 0.7287 - prec: 0.5677 - rec: 0.5794 - val_auc: 0.7047 - val_loss: 0.6233 - val_prec: 0.6266 - val_rec: 0.7330 - learning_rate: 0.0020\n",
      "Epoch 4/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - auc: 0.5841 - loss: 0.7328 - prec: 0.5675 - rec: 0.5643\n",
      "Epoch 4: val_loss improved from 0.62326 to 0.59884, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 242ms/step - auc: 0.6234 - loss: 0.7143 - prec: 0.5949 - rec: 0.5927 - val_auc: 0.7337 - val_loss: 0.5988 - val_prec: 0.6476 - val_rec: 0.7136 - learning_rate: 0.0020\n",
      "Epoch 5/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - auc: 0.6918 - loss: 0.6615 - prec: 0.6436 - rec: 0.6293\n",
      "Epoch 5: val_loss improved from 0.59884 to 0.57633, saving model to models/V1_StackedBiGRU_Attn_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 246ms/step - auc: 0.7290 - loss: 0.6471 - prec: 0.6647 - rec: 0.6800 - val_auc: 0.8164 - val_loss: 0.5763 - val_prec: 0.6691 - val_rec: 0.8932 - learning_rate: 0.0020\n",
      "Epoch 6/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - auc: 0.8747 - loss: 0.4563 - prec: 0.8110 - rec: 0.8151\n",
      "Epoch 6: val_loss did not improve from 0.57633\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 238ms/step - auc: 0.9030 - loss: 0.4181 - prec: 0.8394 - rec: 0.8364 - val_auc: 0.8204 - val_loss: 0.7510 - val_prec: 0.8400 - val_rec: 0.4078 - learning_rate: 0.0020\n",
      "Epoch 7/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - auc: 0.9620 - loss: 0.2754 - prec: 0.9037 - rec: 0.8948\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.57633\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 240ms/step - auc: 0.9518 - loss: 0.3019 - prec: 0.8873 - rec: 0.8970 - val_auc: 0.8048 - val_loss: 0.7920 - val_prec: 0.7821 - val_rec: 0.5922 - learning_rate: 0.0020\n",
      "Epoch 8/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - auc: 0.9928 - loss: 0.1179 - prec: 0.9841 - rec: 0.9438\n",
      "Epoch 8: val_loss did not improve from 0.57633\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 233ms/step - auc: 0.9895 - loss: 0.1306 - prec: 0.9751 - rec: 0.9503 - val_auc: 0.8108 - val_loss: 0.8719 - val_prec: 0.7064 - val_rec: 0.8058 - learning_rate: 0.0010\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "[V1_StackedBiGRU_Attn] SEQ_LEN=200 BATCH=32 drop=0.35 | Val F1(macro)=0.7472 @ thr=0.545\n",
      "Epoch 1/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - auc: 0.6008 - loss: 0.7594 - prec: 0.5618 - rec: 0.5735\n",
      "Epoch 1: val_loss improved from None to 0.58619, saving model to models/V2_CNN_BiGRU_MHA_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 225ms/step - auc: 0.6186 - loss: 0.7450 - prec: 0.5749 - rec: 0.6097 - val_auc: 0.7667 - val_loss: 0.5862 - val_prec: 0.7178 - val_rec: 0.7039 - learning_rate: 0.0020\n",
      "Epoch 2/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - auc: 0.7371 - loss: 0.6349 - prec: 0.6738 - rec: 0.6629\n",
      "Epoch 2: val_loss improved from 0.58619 to 0.56041, saving model to models/V2_CNN_BiGRU_MHA_best.keras\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 191ms/step - auc: 0.7826 - loss: 0.5945 - prec: 0.7192 - rec: 0.7079 - val_auc: 0.8287 - val_loss: 0.5604 - val_prec: 0.7212 - val_rec: 0.7913 - learning_rate: 0.0020\n",
      "Epoch 3/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - auc: 0.9412 - loss: 0.3276 - prec: 0.8602 - rec: 0.8948\n",
      "Epoch 3: val_loss did not improve from 0.56041\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 187ms/step - auc: 0.9371 - loss: 0.3401 - prec: 0.8648 - rec: 0.8764 - val_auc: 0.8438 - val_loss: 0.5649 - val_prec: 0.7628 - val_rec: 0.7961 - learning_rate: 0.0020\n",
      "Epoch 4/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - auc: 0.9880 - loss: 0.1606 - prec: 0.9613 - rec: 0.9438\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.56041\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 192ms/step - auc: 0.9847 - loss: 0.1663 - prec: 0.9539 - rec: 0.9539 - val_auc: 0.8099 - val_loss: 0.8189 - val_prec: 0.7379 - val_rec: 0.7379 - learning_rate: 0.0020\n",
      "Epoch 5/14\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - auc: 0.9963 - loss: 0.0790 - prec: 0.9818 - rec: 0.9729\n",
      "Epoch 5: val_loss did not improve from 0.56041\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 187ms/step - auc: 0.9959 - loss: 0.0836 - prec: 0.9805 - rec: 0.9733 - val_auc: 0.8012 - val_loss: 1.0029 - val_prec: 0.7174 - val_rec: 0.8010 - learning_rate: 0.0010\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "[V2_CNN_BiGRU_MHA] SEQ_LEN=200 BATCH=32 drop=0.3 | Val F1(macro)=0.7551 @ thr=0.695\n",
      "\n",
      "=== BEST CONFIG ===\n",
      "{'variant': 'V2_CNN_BiGRU_MHA', 'f1': 0.7682704685109174, 'thr': 0.6299999999999999, 'seq_len': 200, 'batch': 64, 'dropout': 0.35, 'model': <Functional name=V2_CNN_BiGRU_MHA, built=True>}\n",
      "Predicting test with best model …\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextVectorization' object has no attribute 'set_output_sequence_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 420\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPredicting test with best model …\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    419\u001b[39m \u001b[38;5;66;03m# Re-tokenise test with current SEQ_LEN\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[43mtext_vec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_output_sequence_length\u001b[49m(best[\u001b[33m\"\u001b[39m\u001b[33mseq_len\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    421\u001b[39m X_te_tok = text_vec(tf.constant(X_test_text))  \u001b[38;5;66;03m# EagerTensor OK for predict\u001b[39;00m\n\u001b[32m    422\u001b[39m test_probs = best[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m].predict([X_te_tok, feats_te], batch_size=best[\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m], verbose=\u001b[32m0\u001b[39m).ravel()\n",
      "\u001b[31mAttributeError\u001b[39m: 'TextVectorization' object has no attribute 'set_output_sequence_length'"
     ]
    }
   ],
   "source": [
    "# 06_neural_advanced.ipynb — Task 3.2\n",
    "# Advanced neural variants (Keras) with dual-run support (local + Kaggle)\n",
    "# - Loads data (auto-detect)\n",
    "# - Reuses engineered features from 03_feature_engineering\n",
    "# - Two model variants: Stacked BiGRU + Attention, and CNN+BiGRU+MultiHeadAttention\n",
    "# - Mini hyperparam sweep (seq_len, batch, dropout)\n",
    "# - Threshold tune (macro-F1), write Kaggle /kaggle/working/submission.csv and local submissions/submission.csv\n",
    "\n",
    "# ========= 0) Imports & environment =========\n",
    "import os, sys, re, glob, json, math, warnings\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"NumPy :\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "KAGGLE_DIR = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "KAGGLE_WORKING = \"/kaggle/working\" if IS_KAGGLE else None\n",
    "OUT_KAGGLE = os.path.join(KAGGLE_WORKING, \"submission.csv\") if IS_KAGGLE else None\n",
    "\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "OUT_LOCAL = \"submissions/submission.csv\"\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# ========= 1) Require TensorFlow (Kaggle TF image or local TF install) =========\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    print(\"TensorFlow:\", tf.__version__)\n",
    "    print(\"Devices:\", tf.config.list_physical_devices())\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"TensorFlow is required for Task 3.2.\\n\"\n",
    "        \"On Kaggle: set Image=TensorFlow, Accelerator=None/CPU is fine. Internet OFF.\\n\"\n",
    "        \"On Mac (Apple Silicon): use tensorflow-macos 2.16.x + tensorflow-metal.\\n\"\n",
    "        f\"Import error: {e}\"\n",
    "    )\n",
    "\n",
    "# Optional: quieter logs\n",
    "import logging\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\",\"2\")\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.config.set_soft_device_placement(True)\n",
    "\n",
    "# ========= 2) Data loaders (Kaggle-first, then local fallbacks) =========\n",
    "CANDIDATE_DIRS = [\n",
    "    \".\", \"data/raw\", \"../data/raw\", \"../../data/raw\",\n",
    "    \"jigsaw-agile-community-rules\", \"../jigsaw-agile-community-rules\", \"../../jigsaw-agile-community-rules\"\n",
    "]\n",
    "def _candidate_paths(filename: str):\n",
    "    paths = []\n",
    "    if os.path.exists(KAGGLE_DIR):\n",
    "        paths.append(os.path.join(KAGGLE_DIR, filename))\n",
    "    for d in CANDIDATE_DIRS:\n",
    "        paths.append(os.path.join(d, filename))\n",
    "    paths.extend(glob.glob(f\"**/{filename}\", recursive=True))\n",
    "    seen, out = set(), []\n",
    "    for p in paths:\n",
    "        ap = os.path.abspath(p)\n",
    "        if ap not in seen and os.path.exists(ap):\n",
    "            seen.add(ap); out.append(ap)\n",
    "    return out\n",
    "\n",
    "def read_first_csv(filename: str):\n",
    "    found = _candidate_paths(filename)\n",
    "    if not found:\n",
    "        raise FileNotFoundError(f\"Could not find {filename} in Kaggle folder or local fallbacks.\")\n",
    "    print(f\"📄 Loading {filename} from: {found[0]}\")\n",
    "    return pd.read_csv(found[0])\n",
    "\n",
    "train_df = read_first_csv(\"train.csv\")\n",
    "test_df  = read_first_csv(\"test.csv\")\n",
    "sample   = read_first_csv(\"sample_submission.csv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test  shape:\", test_df.shape)\n",
    "print(\"Sample shape:\", sample.shape)\n",
    "\n",
    "# ========= 3) Column detection =========\n",
    "TEXT_COL = next((c for c in [\"comment_text\",\"body\",\"text\"] if c in train_df.columns), None)\n",
    "TARGET_COL = next((c for c in [\"rule_violation\",\"target\",\"label\"] if c in train_df.columns), None)\n",
    "ID_COL, TARGET_OUT = sample.columns[0], sample.columns[1]\n",
    "assert TEXT_COL and TARGET_COL, \"Expected text and target columns.\"\n",
    "\n",
    "print(f\"TEXT_COL  = {TEXT_COL}\")\n",
    "print(f\"TARGET_COL= {TARGET_COL}\")\n",
    "print(f\"ID_COL    = {ID_COL} | TARGET_OUT = {TARGET_OUT}\")\n",
    "\n",
    "# ========= 4) Data prep & split =========\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "X_text = train_df[TEXT_COL].fillna(\"\").astype(str).values\n",
    "y = train_df[TARGET_COL].astype(int).values\n",
    "X_test_text = test_df[TEXT_COL].fillna(\"\").astype(str).values\n",
    "\n",
    "X_tr_text, X_va_text, y_tr, y_va = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "def compute_class_weights(y_array):\n",
    "    n = len(y_array); pos = int((y_array==1).sum()); neg = n - pos\n",
    "    return {0: n/(2*neg), 1: n/(2*pos)}\n",
    "CLASS_WEIGHTS = compute_class_weights(y_tr)\n",
    "print(\"Class weights:\", CLASS_WEIGHTS)\n",
    "\n",
    "# ========= 5) Load engineered features from Task 2 (or compute if missing) =========\n",
    "import os, re, numpy as np, pandas as pd\n",
    "\n",
    "# write to both local project path and Kaggle-working so later notebooks can see them\n",
    "PKL_DIRS = [\"data/processed\", \"/kaggle/working/data/processed\"] if os.path.exists(\"/kaggle\") else [\"data/processed\"]\n",
    "for d in PKL_DIRS: os.makedirs(d, exist_ok=True)\n",
    "\n",
    "TR_FEATS_PKL = \"data/processed/train_features.pkl\"\n",
    "TE_FEATS_PKL = \"data/processed/test_features.pkl\"\n",
    "\n",
    "def _extract_features_minimal(df, text_col):\n",
    "    s = df[text_col].fillna(\"\").astype(str)\n",
    "    # helpers\n",
    "    def ratio_safe(num, den):\n",
    "        den = np.maximum(den, 1)\n",
    "        return num / den\n",
    "\n",
    "    # counts\n",
    "    char_count = s.str.len().astype(np.int32)\n",
    "    word_count = s.str.split().str.len().fillna(0).astype(np.int32)\n",
    "    uniq_word_count = s.apply(lambda x: len(set(x.split())) if x else 0).astype(np.int32)\n",
    "    avg_word_len = ratio_safe(char_count, word_count).astype(np.float32)\n",
    "\n",
    "    # casings & punctuation\n",
    "    caps_count = s.str.count(r\"[A-Z]\")\n",
    "    caps_ratio = ratio_safe(caps_count, char_count).astype(np.float32)\n",
    "    all_caps_words = s.str.count(r\"\\b[A-Z]{2,}\\b\").astype(np.int32)\n",
    "\n",
    "    excl_count = s.str.count(\"!\")\n",
    "    ques_count = s.str.count(r\"\\?\")\n",
    "    dots3_count = s.str.count(r\"\\.\\.\\.\")\n",
    "    punct_count = s.str.count(r\"[^\\w\\s]\")  # crude punctuation\n",
    "    punct_ratio = ratio_safe(punct_count, char_count).astype(np.float32)\n",
    "    excl_ratio = ratio_safe(excl_count, char_count).astype(np.float32)\n",
    "    ques_ratio = ratio_safe(ques_count, char_count).astype(np.float32)\n",
    "\n",
    "    # reddit / markup / links / quotes\n",
    "    has_user = s.str.contains(r\"u/\\w+\", regex=True).astype(np.int8)\n",
    "    has_sub  = s.str.contains(r\"r/\\w+\", regex=True).astype(np.int8)\n",
    "    has_url  = s.str.contains(r\"http\", regex=True).astype(np.int8)\n",
    "    quote_count = s.str.count(r\"^>|\\n>\", flags=re.MULTILINE).astype(np.int32)\n",
    "    md_links = s.str.count(r\"\\[[^\\]]+\\]\\([^)]+\\)\").astype(np.int32)\n",
    "\n",
    "    # lexical tokens\n",
    "    you_count = s.str.count(r\"\\byou\\b\", flags=re.IGNORECASE).astype(np.int32)\n",
    "    i_count   = s.str.count(r\"\\bi\\b\", flags=re.IGNORECASE).astype(np.int32)\n",
    "    num_count = s.str.count(r\"\\d\").astype(np.int32)\n",
    "    negate_count = s.str.count(r\"\\b(no|not|never|n't)\\b\", flags=re.IGNORECASE).astype(np.int32)\n",
    "\n",
    "    you_ratio = ratio_safe(you_count, word_count).astype(np.float32)\n",
    "    i_ratio   = ratio_safe(i_count, word_count).astype(np.float32)\n",
    "    num_ratio = ratio_safe(num_count, word_count).astype(np.float32)\n",
    "\n",
    "    # lexical diversity (unique / total)\n",
    "    lexical_diversity = ratio_safe(uniq_word_count, word_count).astype(np.float32)\n",
    "\n",
    "    feats = pd.DataFrame({\n",
    "        \"char_count\": char_count,\n",
    "        \"word_count\": word_count,\n",
    "        \"uniq_word_count\": uniq_word_count,\n",
    "        \"avg_word_len\": avg_word_len,\n",
    "        \"caps_ratio\": caps_ratio,\n",
    "        \"all_caps_words\": all_caps_words,\n",
    "        \"excl_count\": excl_count,\n",
    "        \"ques_count\": ques_count,\n",
    "        \"dots3_count\": dots3_count,\n",
    "        \"punct_count\": punct_count,\n",
    "        \"punct_ratio\": punct_ratio,\n",
    "        \"excl_ratio\": excl_ratio,\n",
    "        \"ques_ratio\": ques_ratio,\n",
    "        \"has_user_mention\": has_user,\n",
    "        \"has_subreddit_mention\": has_sub,\n",
    "        \"has_url\": has_url,\n",
    "        \"quote_count\": quote_count,\n",
    "        \"md_links\": md_links,\n",
    "        \"you_count\": you_count,\n",
    "        \"i_count\": i_count,\n",
    "        \"num_count\": num_count,\n",
    "        \"negate_count\": negate_count,\n",
    "        \"you_ratio\": you_ratio,\n",
    "        \"i_ratio\": i_ratio,\n",
    "        \"num_ratio\": num_ratio,\n",
    "        \"lexical_diversity\": lexical_diversity,\n",
    "    }).astype({\n",
    "        # keep dtypes tight where possible\n",
    "        \"all_caps_words\":\"int32\",\"excl_count\":\"int32\",\"ques_count\":\"int32\",\"dots3_count\":\"int32\",\n",
    "        \"punct_count\":\"int32\",\"quote_count\":\"int32\",\"md_links\":\"int32\",\"you_count\":\"int32\",\n",
    "        \"i_count\":\"int32\",\"num_count\":\"int32\",\"negate_count\":\"int32\",\n",
    "        \"has_user_mention\":\"int8\",\"has_subreddit_mention\":\"int8\",\"has_url\":\"int8\",\n",
    "    })\n",
    "\n",
    "    # Ensure consistent column order\n",
    "    return feats.reindex(sorted(feats.columns), axis=1)\n",
    "\n",
    "# Load or compute\n",
    "if os.path.exists(TR_FEATS_PKL) and os.path.exists(TE_FEATS_PKL):\n",
    "    train_features = pd.read_pickle(TR_FEATS_PKL)\n",
    "    test_features  = pd.read_pickle(TE_FEATS_PKL)\n",
    "    print(\"✅ Loaded engineered features from 03.\")\n",
    "else:\n",
    "    print(\"⚙️  Computing engineered features (03 fallback)…\")\n",
    "    train_features = _extract_features_minimal(train_df, TEXT_COL)\n",
    "    test_features  = _extract_features_minimal(test_df,  TEXT_COL)\n",
    "\n",
    "    # Save to both places so future runs find them\n",
    "    for base in PKL_DIRS:\n",
    "        pf_tr = os.path.join(base, \"train_features.pkl\")\n",
    "        pf_te = os.path.join(base, \"test_features.pkl\")\n",
    "        train_features.to_pickle(pf_tr)\n",
    "        test_features.to_pickle(pf_te)\n",
    "        print(\"💾 Saved:\", pf_tr, \"and\", pf_te)\n",
    "\n",
    "print(f\"Features ready. Train: {train_features.shape} | Test: {test_features.shape}\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "feat_scaler = StandardScaler()\n",
    "feats_tr_all = feat_scaler.fit_transform(train_features.values.astype(np.float32))\n",
    "feats_te     = feat_scaler.transform(test_features.values.astype(np.float32))\n",
    "\n",
    "# keep split consistent\n",
    "X_idx = np.arange(len(X_text))\n",
    "idx_tr, idx_va, _, _ = train_test_split(X_idx, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "feats_tr_split = feats_tr_all[idx_tr]\n",
    "feats_va_split = feats_tr_all[idx_va]\n",
    "\n",
    "# ========= 6) FAST_DEBUG toggle + TextVectorization =========\n",
    "FAST_DEBUG = False  # set True to smoke-test quickly\n",
    "MAX_TOKENS = 30000\n",
    "SEQ_LEN = 160 if FAST_DEBUG else 200\n",
    "HAS_GPU = bool(tf.config.list_physical_devices('GPU'))\n",
    "BATCH = 32 if FAST_DEBUG or not HAS_GPU else 64\n",
    "EPOCHS = 4 if FAST_DEBUG else 14  # early stopping will cap earlier\n",
    "\n",
    "print(f\"[FAST_DEBUG={FAST_DEBUG}] HAS_GPU={HAS_GPU} | SEQ_LEN={SEQ_LEN} | BATCH={BATCH} | EPOCHS={EPOCHS}\")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "text_vec = keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS, output_mode=\"int\", output_sequence_length=SEQ_LEN,\n",
    "    standardize=\"lower_and_strip_punctuation\", split=\"whitespace\"\n",
    ")\n",
    "text_vec.adapt(tf.data.Dataset.from_tensor_slices(X_tr_text).batch(256))\n",
    "\n",
    "# ========= 7) tf.data with auxiliary features =========\n",
    "def make_ds_with_feats(x_arr, feats_arr, y_arr=None, train=False):\n",
    "    if y_arr is None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((x_arr, feats_arr))\n",
    "        if train: ds = ds.shuffle(len(x_arr), seed=SEED)\n",
    "        ds = ds.batch(BATCH).map(lambda txt,f: (text_vec(txt), f), num_parallel_calls=AUTOTUNE)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((x_arr, feats_arr, y_arr))\n",
    "        if train: ds = ds.shuffle(len(x_arr), seed=SEED)\n",
    "        ds = ds.batch(BATCH).map(lambda txt,f,y: ((text_vec(txt), f), y), num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "ds_tr = make_ds_with_feats(X_tr_text, feats_tr_split, y_tr, train=True)\n",
    "ds_va = make_ds_with_feats(X_va_text, feats_va_split, y_va, train=False)\n",
    "\n",
    "# sanity\n",
    "for batch in ds_tr.take(1):\n",
    "    (tok_ids, aux_feats), yb = batch\n",
    "    print(\"Sample batch:\", tok_ids.shape, aux_feats.shape, yb.shape)\n",
    "\n",
    "# ========= 8) Model builders (two variants) =========\n",
    "L2 = keras.regularizers.l2(1e-5)\n",
    "\n",
    "def build_v1_stacked_bigru_attn(vocab_size, n_aux, embed_dim=128, gru_units=96, dropout=0.35):\n",
    "    tok_in = keras.Input(shape=(SEQ_LEN,), dtype=\"int32\", name=\"tok_ids\")\n",
    "    aux_in = keras.Input(shape=(n_aux,), dtype=\"float32\", name=\"aux_feats\")\n",
    "\n",
    "    emb = keras.layers.Embedding(vocab_size, embed_dim, mask_zero=False,\n",
    "                                 embeddings_regularizer=L2, name=\"emb\")(tok_in)\n",
    "\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(gru_units, return_sequences=True, dropout=0.2), name=\"bigru_1\")(emb)\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(gru_units//2, return_sequences=True, dropout=0.2), name=\"bigru_2\")(x)\n",
    "\n",
    "    # Lightweight attention: Query from pooled context over Keys/Values = x\n",
    "    q = keras.layers.GlobalAveragePooling1D(name=\"attn_query_pool\")(x)\n",
    "    q = keras.layers.Reshape((1, q.shape[-1]))(q)\n",
    "    attn_out = keras.layers.Attention(name=\"attn\")([q, x])  # shape (None,1,d)\n",
    "    attn_out = keras.layers.Flatten()(attn_out)\n",
    "\n",
    "    aux_h = keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=L2)(aux_in)\n",
    "    aux_h = keras.layers.Dropout(0.2)(aux_h)\n",
    "\n",
    "    h = keras.layers.Concatenate(name=\"fusion\")([attn_out, aux_h])\n",
    "    h = keras.layers.Dropout(dropout)(h)\n",
    "    h = keras.layers.Dense(192, activation=\"relu\", kernel_regularizer=L2)(h)\n",
    "    h = keras.layers.Dropout(dropout)(h)\n",
    "    out = keras.layers.Dense(1, activation=\"sigmoid\", name=\"out\")(h)\n",
    "\n",
    "    model = keras.Model([tok_in, aux_in], out, name=\"V1_StackedBiGRU_Attn\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(2e-3),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[keras.metrics.AUC(name=\"auc\"),\n",
    "                           keras.metrics.Precision(name=\"prec\"),\n",
    "                           keras.metrics.Recall(name=\"rec\")])\n",
    "    return model\n",
    "\n",
    "def build_v2_cnn_bigru_mha(vocab_size, n_aux, embed_dim=128, gru_units=96, dropout=0.35, heads=4):\n",
    "    tok_in = keras.Input(shape=(SEQ_LEN,), dtype=\"int32\", name=\"tok_ids\")\n",
    "    aux_in = keras.Input(shape=(n_aux,), dtype=\"float32\", name=\"aux_feats\")\n",
    "\n",
    "    emb = keras.layers.Embedding(vocab_size, embed_dim, mask_zero=False,\n",
    "                                 embeddings_regularizer=L2, name=\"emb\")(tok_in)\n",
    "\n",
    "    # CNN path\n",
    "    c = keras.layers.SpatialDropout1D(0.2)(emb)\n",
    "    c = keras.layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\", kernel_regularizer=L2)(c)\n",
    "    c = keras.layers.Conv1D(128, 5, padding=\"same\", activation=\"relu\", kernel_regularizer=L2)(c)\n",
    "    gmp, gap = keras.layers.GlobalMaxPooling1D()(c), keras.layers.GlobalAveragePooling1D()(c)\n",
    "    c = keras.layers.Concatenate()([gmp, gap])\n",
    "\n",
    "    # BiGRU with MHA\n",
    "    r = keras.layers.Bidirectional(keras.layers.GRU(gru_units, return_sequences=True, dropout=0.2))(emb)\n",
    "    r = keras.layers.MultiHeadAttention(num_heads=heads, key_dim=embed_dim//heads, dropout=0.1)(r, r)\n",
    "    r = keras.layers.GlobalMaxPooling1D()(r)\n",
    "\n",
    "    aux_h = keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=L2)(aux_in)\n",
    "    aux_h = keras.layers.Dropout(0.2)(aux_h)\n",
    "\n",
    "    h = keras.layers.Concatenate()([c, r, aux_h])\n",
    "    h = keras.layers.Dropout(dropout)(h)\n",
    "    h = keras.layers.Dense(192, activation=\"relu\", kernel_regularizer=L2)(h)\n",
    "    h = keras.layers.Dropout(dropout)(h)\n",
    "    out = keras.layers.Dense(1, activation=\"sigmoid\", name=\"out\")(h)\n",
    "\n",
    "    model = keras.Model([tok_in, aux_in], out, name=\"V2_CNN_BiGRU_MHA\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(2e-3),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[keras.metrics.AUC(name=\"auc\"),\n",
    "                           keras.metrics.Precision(name=\"prec\"),\n",
    "                           keras.metrics.Recall(name=\"rec\")])\n",
    "    return model\n",
    "\n",
    "# ========= 9) Tiny hyperparam sweep driver =========\n",
    "VOCAB_SIZE = MAX_TOKENS + 2\n",
    "N_AUX = feats_tr_all.shape[1]\n",
    "\n",
    "grid = [\n",
    "    # (variant, seq_len, batch, dropout)\n",
    "    (\"V1_StackedBiGRU_Attn\", SEQ_LEN, BATCH, 0.35),\n",
    "    (\"V2_CNN_BiGRU_MHA\",    SEQ_LEN, BATCH, 0.35),\n",
    "]\n",
    "# Optionally expand grid when not in FAST_DEBUG:\n",
    "if not FAST_DEBUG:\n",
    "    grid += [\n",
    "        (\"V1_StackedBiGRU_Attn\", SEQ_LEN, max(16, BATCH//2), 0.35),\n",
    "        (\"V2_CNN_BiGRU_MHA\",    SEQ_LEN, max(16, BATCH//2), 0.30),\n",
    "    ]\n",
    "\n",
    "def train_and_eval(variant, seq_len, batch, dropout):\n",
    "    global SEQ_LEN, BATCH\n",
    "    SEQ_LEN, BATCH = seq_len, batch\n",
    "\n",
    "    # Rebuild datasets with the new SEQ_LEN/BATCH\n",
    "    ds_tr_local = make_ds_with_feats(X_tr_text, feats_tr_split, y_tr, train=True)\n",
    "    ds_va_local = make_ds_with_feats(X_va_text, feats_va_split, y_va, train=False)\n",
    "\n",
    "    # Build model\n",
    "    if variant == \"V1_StackedBiGRU_Attn\":\n",
    "        model = build_v1_stacked_bigru_attn(VOCAB_SIZE, N_AUX, embed_dim=128, gru_units=96, dropout=dropout)\n",
    "    else:\n",
    "        model = build_v2_cnn_bigru_mha(VOCAB_SIZE, N_AUX, embed_dim=128, gru_units=96, dropout=dropout, heads=4)\n",
    "\n",
    "    # Warm-up (compile kernels)\n",
    "    _ = model.predict(ds_tr_local.take(1), verbose=0)\n",
    "\n",
    "    ckpt_dir = (KAGGLE_WORKING if IS_KAGGLE else \"models\")\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    ckpt_path = os.path.join(ckpt_dir, f\"{variant}_best.keras\")\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=5e-5, verbose=1),\n",
    "        keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_loss\", save_best_only=True, verbose=1)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        ds_tr_local, validation_data=ds_va_local, epochs=EPOCHS,\n",
    "        class_weight=CLASS_WEIGHTS, callbacks=callbacks, verbose=1\n",
    "    )\n",
    "\n",
    "    # Tune threshold on validation\n",
    "    va_probs = model.predict(ds_va_local, verbose=0).ravel()\n",
    "    thr_grid = np.linspace(0.30, 0.70, 81)\n",
    "    f1s = [f1_score(y_va, (va_probs >= t).astype(int), average=\"macro\") for t in thr_grid]\n",
    "    best_idx = int(np.argmax(f1s))\n",
    "    best_thr = float(thr_grid[best_idx])\n",
    "    val_f1 = float(f1s[best_idx])\n",
    "\n",
    "    print(f\"[{variant}] SEQ_LEN={seq_len} BATCH={batch} drop={dropout} | Val F1(macro)={val_f1:.4f} @ thr={best_thr:.3f}\")\n",
    "    return model, val_f1, best_thr\n",
    "\n",
    "# Run the sweep and keep the best\n",
    "best = {\"variant\": None, \"f1\": -1.0, \"thr\": 0.5, \"seq_len\": SEQ_LEN, \"batch\": BATCH, \"dropout\": 0.35, \"model\": None}\n",
    "for (v, sl, bs, dr) in grid:\n",
    "    model, f1v, thr = train_and_eval(v, sl, bs, dr)\n",
    "    if f1v > best[\"f1\"]:\n",
    "        best.update({\"variant\": v, \"f1\": f1v, \"thr\": thr, \"seq_len\": sl, \"batch\": bs, \"dropout\": dr, \"model\": model})\n",
    "\n",
    "print(\"\\n=== BEST CONFIG ===\")\n",
    "print(best)\n",
    "\n",
    "# ========= 10) Predict test explicitly (two-input) & build submission =========\n",
    "print(\"Predicting test with best model …\")\n",
    "# Re-tokenise test with current SEQ_LEN\n",
    "text_vec.set_output_sequence_length(best[\"seq_len\"])\n",
    "X_te_tok = text_vec(tf.constant(X_test_text))  # EagerTensor OK for predict\n",
    "test_probs = best[\"model\"].predict([X_te_tok, feats_te], batch_size=best[\"batch\"], verbose=0).ravel()\n",
    "test_pred  = (test_probs >= best[\"thr\"]).astype(int)\n",
    "\n",
    "submission = sample.copy()\n",
    "submission[TARGET_OUT] = test_pred.astype(int)\n",
    "\n",
    "# Validate submission\n",
    "errors = []\n",
    "if list(submission.columns) != list(sample.columns):\n",
    "    errors.append(f\"Columns mismatch. Expected {list(sample.columns)}, got {list(submission.columns)}\")\n",
    "if len(submission) != len(sample):\n",
    "    errors.append(f\"Row count mismatch. Expected {len(sample)}, got {len(submission)}\")\n",
    "if not submission[ID_COL].equals(sample[ID_COL]):\n",
    "    if set(submission[ID_COL]) != set(sample[ID_COL]):\n",
    "        missing = list(sorted(set(sample[ID_COL]) - set(submission[ID_COL])))[:5]\n",
    "        extra   = list(sorted(set(submission[ID_COL]) - set(sample[ID_COL])))[:5]\n",
    "        errors.append(f\"ID set differs. Missing: {missing} | Extra: {extra}\")\n",
    "    else:\n",
    "        errors.append(\"ID order differs from sample. Must match sample_submission order.\")\n",
    "if submission[TARGET_OUT].isna().any():\n",
    "    errors.append(\"Target has NaNs.\")\n",
    "u = set(np.unique(submission[TARGET_OUT]))\n",
    "if not u.issubset({0,1}):\n",
    "    errors.append(f\"Target invalid values {sorted(u)}; must be 0/1.\")\n",
    "if errors:\n",
    "    print(\"❌ Submission invalid:\"); [print(\" -\", e) for e in errors]; raise SystemExit(1)\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    submission.to_csv(OUT_KAGGLE, index=False)\n",
    "    print(f\"✅ Saved Kaggle file: {OUT_KAGGLE}\")\n",
    "submission.to_csv(OUT_LOCAL, index=False)\n",
    "print(f\"✅ Saved local copy : {OUT_LOCAL}\")\n",
    "\n",
    "# ========= 11) Log run info =========\n",
    "run_info = {\n",
    "    \"task\": \"3.2_neural_advanced\",\n",
    "    \"best\": {\n",
    "        \"variant\": best[\"variant\"],\n",
    "        \"val_f1_macro\": float(best[\"f1\"]),\n",
    "        \"threshold\": float(best[\"thr\"]),\n",
    "        \"seq_len\": int(best[\"seq_len\"]),\n",
    "        \"batch\": int(best[\"batch\"]),\n",
    "        \"dropout\": float(best[\"dropout\"]),\n",
    "    },\n",
    "    \"seed\": SEED,\n",
    "    \"time\": datetime.now().isoformat(timespec=\"seconds\")\n",
    "}\n",
    "with open(\"results/run_06_neural_advanced.json\",\"w\") as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "\n",
    "print(\"\\nModel used:\", best[\"variant\"])\n",
    "print(f\"Validation F1 (macro): {best['f1']:.4f} at threshold {best['thr']:.3f}\")\n",
    "print(\"Final submission head:\\n\", submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eef7ba-218c-43f9-ac99-ffde595e4650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac-py312)",
   "language": "python",
   "name": "tf-mac-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
