{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7eb911f-18fb-4aa4-9e4f-4ce80908e2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\n",
      "NumPy : 1.26.4\n",
      "Pandas: 2.2.3\n",
      "TensorFlow: 2.16.2\n",
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "ğŸ“„ Loading train.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/train.csv\n",
      "ğŸ“„ Loading test.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/test.csv\n",
      "ğŸ“„ Loading sample_submission.csv from: /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/sample_submission.csv\n",
      "Train shape: (2029, 9)\n",
      "Test  shape: (10, 8)\n",
      "Sample shape: (10, 2)\n",
      "TEXT_COL  = body\n",
      "TARGET_COL= rule_violation\n",
      "ID_COL    = row_id | TARGET_OUT = rule_violation\n",
      "Class weights: {0: 1.0169172932330828, 1: 0.9836363636363636}\n",
      "Step 5: Building TextVectorizationâ€¦\n",
      "  HAS_GPU=True | BATCH=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 13:26:22.710511: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2025-09-19 13:26:22.710538: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2025-09-19 13:26:22.710542: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 12.48 GB\n",
      "2025-09-19 13:26:22.710586: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-19 13:26:22.710612: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-09-19 13:26:22.876119: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TextVectorization adapted.\n",
      "Step 6: Loading / computing engineered featuresâ€¦\n",
      "  Loaded cached features.\n",
      "  Feats shapes: (1623, 42) (406, 42) (10, 42)\n",
      "Step 7: Building tf.data with auxiliary featuresâ€¦\n",
      "  tf.data ready.\n",
      "Sample batch: (64, 200) (64, 42) (64,)\n",
      "Step 8: Building modelâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 13:26:23.079124: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up: building graph with a single batch â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 13:26:23.535023: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-09-19 13:26:23.563748: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up predict done. Starting trainingâ€¦\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"hybrid_text_feats\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"hybrid_text_feats\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ tok_ids             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,256</span> â”‚ tok_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ spatial_dropout     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> â”‚ spatial_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> â”‚ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ aux_feats           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bigru_seq           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)  â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">130,176</span> â”‚ emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ cnn_gmp             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1â€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ cnn_gap             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,752</span> â”‚ aux_feats[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ rnn_gmp             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bigru_seq[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1â€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ cnn_pool            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ cnn_gmp[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ cnn_gap[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ fusion              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ rnn_gmp[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ cnn_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ fusion[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,496</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> â”‚ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ tok_ids             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ emb (\u001b[38;5;33mEmbedding\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  â”‚  \u001b[38;5;34m3,840,256\u001b[0m â”‚ tok_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ spatial_dropout     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â”‚ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  â”‚     \u001b[38;5;34m49,280\u001b[0m â”‚ spatial_dropout[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  â”‚     \u001b[38;5;34m82,048\u001b[0m â”‚ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ aux_feats           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bigru_seq           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m192\u001b[0m)  â”‚    \u001b[38;5;34m130,176\u001b[0m â”‚ emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â”‚ (\u001b[38;5;33mBidirectional\u001b[0m)     â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ cnn_gmp             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalMaxPooling1â€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ cnn_gap             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m2,752\u001b[0m â”‚ aux_feats[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ rnn_gmp             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ bigru_seq[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalMaxPooling1â€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ cnn_pool            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ cnn_gmp[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    â”‚\n",
       "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ cnn_gap[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ fusion              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ rnn_gmp[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    â”‚\n",
       "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ cnn_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ fusion[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       â”‚     \u001b[38;5;34m98,496\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ out (\u001b[38;5;33mDense\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         â”‚        \u001b[38;5;34m193\u001b[0m â”‚ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,203,201</span> (16.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,203,201\u001b[0m (16.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,203,201</span> (16.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,203,201\u001b[0m (16.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: Trainingâ€¦\n",
      "Epoch 1/16\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125s/step - auc: 0.5607 - loss: 0.7281 - prec: 0.5601 - rec: 0.5746  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 14:20:38.567396: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from None to 0.62050, saving model to models/tf_neural_baseline_v2.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3280s\u001b[0m 126s/step - auc: 0.6207 - loss: 0.7011 - prec: 0.5965 - rec: 0.6145 - val_auc: 0.7356 - val_loss: 0.6205 - val_prec: 0.6446 - val_rec: 0.7573 - learning_rate: 0.0020\n",
      "Epoch 2/16\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126s/step - auc: 0.7769 - loss: 0.5839 - prec: 0.6964 - rec: 0.8023  \n",
      "Epoch 2: val_loss improved from 0.62050 to 0.56992, saving model to models/tf_neural_baseline_v2.keras\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3317s\u001b[0m 128s/step - auc: 0.8012 - loss: 0.5559 - prec: 0.7081 - rec: 0.7939 - val_auc: 0.8391 - val_loss: 0.5699 - val_prec: 0.8333 - val_rec: 0.5097 - learning_rate: 0.0020\n",
      "Epoch 3/16\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123s/step - auc: 0.9461 - loss: 0.3260 - prec: 0.8999 - rec: 0.8257  \n",
      "Epoch 3: val_loss did not improve from 0.56992\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3225s\u001b[0m 124s/step - auc: 0.9505 - loss: 0.2977 - prec: 0.8953 - rec: 0.8703 - val_auc: 0.8415 - val_loss: 0.5769 - val_prec: 0.7731 - val_rec: 0.8107 - learning_rate: 0.0020\n",
      "Epoch 4/16\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122s/step - auc: 0.9892 - loss: 0.1390 - prec: 0.9571 - rec: 0.9604  \n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.56992\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3203s\u001b[0m 123s/step - auc: 0.9892 - loss: 0.1321 - prec: 0.9624 - rec: 0.9612 - val_auc: 0.8183 - val_loss: 0.8405 - val_prec: 0.7466 - val_rec: 0.8010 - learning_rate: 0.0020\n",
      "Epoch 5/16\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125s/step - auc: 0.9963 - loss: 0.0714 - prec: 0.9943 - rec: 0.9773  \n",
      "Epoch 5: val_loss did not improve from 0.56992\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3286s\u001b[0m 126s/step - auc: 0.9965 - loss: 0.0687 - prec: 0.9902 - rec: 0.9806 - val_auc: 0.8151 - val_loss: 0.8431 - val_prec: 0.7523 - val_rec: 0.8107 - learning_rate: 0.0010\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "  Training done.\n",
      "Step 10: Threshold tuningâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 17:58:41.767512: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best threshold = 0.320 | Val F1(macro) = 0.7755\n",
      "Validation confusion matrix:\n",
      " [[149  51]\n",
      " [ 40 166]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7884    0.7450    0.7661       200\n",
      "           1     0.7650    0.8058    0.7849       206\n",
      "\n",
      "    accuracy                         0.7759       406\n",
      "   macro avg     0.7767    0.7754    0.7755       406\n",
      "weighted avg     0.7765    0.7759    0.7756       406\n",
      "\n",
      "Step 11: Predicting test & writing submissionâ€¦\n",
      "âœ… Saved local copy : submissions/submission.csv\n",
      "Step 12: Logging run infoâ€¦\n",
      "\n",
      "âœ… Done.\n"
     ]
    }
   ],
   "source": [
    "# 05_neural_baseline.ipynb â€” Task 3.1 (TensorFlow required; Kaggle + Local)\n",
    "\n",
    "# - Loads data (auto-detect paths)\n",
    "# - Keras: TextVectorization + Embedding + CNN + BiGRU\n",
    "# - Early stopping + threshold tuning (macro-F1)\n",
    "# - Saves Kaggle-ready /kaggle/working/submission.csv and local submissions/submission.csv\n",
    "\n",
    "# ========= 0) Imports & environment info =========\n",
    "import sys, os, glob, re, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"NumPy :\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "\n",
    "# Output locations\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "KAGGLE_WORKING = \"/kaggle/working\" if IS_KAGGLE else None\n",
    "OUT_KAGGLE = os.path.join(KAGGLE_WORKING, \"submission.csv\") if IS_KAGGLE else None\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "OUT_LOCAL = \"submissions/submission.csv\"\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# ========= 1) Require TensorFlow (fail fast if unavailable) =========\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(\"TensorFlow:\", tf.__version__)\n",
    "    print(\"Devices:\", tf.config.list_physical_devices())\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"TensorFlow is not available in this environment.\\n\"\n",
    "        \"Kaggle: Settings â†’ Environment â†’ set Image=TensorFlow and Accelerator=GPU, Internet=OFF, then Save Version â†’ Run All.\\n\"\n",
    "        \"Local (Apple Silicon): install tensorflow-macos; Local (Intel): install tensorflow==2.15.x.\"\n",
    "    ) from e\n",
    "\n",
    "# Optional: set sensible CPU thread limits to avoid oversubscription on some hosts\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"4\")\n",
    "tf.config.set_soft_device_placement(True)\n",
    "\n",
    "# ========= 2) Locate data (Kaggle-first, then local fallbacks) =========\n",
    "KAGGLE_DIR = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "CANDIDATE_DIRS = [\n",
    "    \".\", \"..\", \"../..\", \"../../..\",\n",
    "    \"data/raw\", \"../data/raw\", \"../../data/raw\",\n",
    "    \"jigsaw-agile-community-rules\", \"../jigsaw-agile-community-rules\"\n",
    "]\n",
    "\n",
    "def _candidate_paths(filename: str):\n",
    "    paths = []\n",
    "    if os.path.exists(KAGGLE_DIR):\n",
    "        paths.append(os.path.join(KAGGLE_DIR, filename))\n",
    "    for d in CANDIDATE_DIRS:\n",
    "        paths.append(os.path.join(d, filename))\n",
    "    paths.extend(glob.glob(f\"**/{filename}\", recursive=True))\n",
    "    # Deduplicate existing\n",
    "    seen, out = set(), []\n",
    "    for p in paths:\n",
    "        ap = os.path.abspath(p)\n",
    "        if ap not in seen and os.path.exists(ap):\n",
    "            seen.add(ap); out.append(ap)\n",
    "    return out\n",
    "\n",
    "def read_first_csv(filename: str):\n",
    "    found = _candidate_paths(filename)\n",
    "    if not found:\n",
    "        raise FileNotFoundError(f\"Could not find {filename} in Kaggle folder or local fallbacks.\")\n",
    "    print(f\"ğŸ“„ Loading {filename} from: {found[0]}\")\n",
    "    return pd.read_csv(found[0])\n",
    "\n",
    "train_df = read_first_csv(\"train.csv\")\n",
    "test_df  = read_first_csv(\"test.csv\")\n",
    "sample   = read_first_csv(\"sample_submission.csv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test  shape:\", test_df.shape)\n",
    "print(\"Sample shape:\", sample.shape)\n",
    "\n",
    "# ========= 3) Robust column detection =========\n",
    "TEXT_COL = next((c for c in [\"comment_text\", \"body\", \"text\"] if c in train_df.columns), None)\n",
    "TARGET_COL = next((c for c in [\"rule_violation\", \"target\", \"label\"] if c in train_df.columns), None)\n",
    "ID_COL = sample.columns[0]\n",
    "TARGET_OUT = sample.columns[1]\n",
    "\n",
    "assert TEXT_COL is not None, \"No text column found (expected comment_text/body/text).\"\n",
    "assert TARGET_COL is not None, \"No target column found (expected rule_violation/target/label).\"\n",
    "print(f\"TEXT_COL  = {TEXT_COL}\")\n",
    "print(f\"TARGET_COL= {TARGET_COL}\")\n",
    "print(f\"ID_COL    = {ID_COL} | TARGET_OUT = {TARGET_OUT}\")\n",
    "\n",
    "# ========= 4) Prepare data & utilities =========\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "X_text = train_df[TEXT_COL].fillna(\"\").astype(str).values\n",
    "y = train_df[TARGET_COL].astype(int).values\n",
    "X_test_text = test_df[TEXT_COL].fillna(\"\").astype(str).values\n",
    "\n",
    "# stratified split for validation\n",
    "X_tr_text, X_va_text, y_tr, y_va = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "def compute_class_weights(y_array):\n",
    "    # balanced weights: N / (2 * count_class)\n",
    "    n = len(y_array)\n",
    "    pos = int((y_array == 1).sum())\n",
    "    neg = n - pos\n",
    "    return {0: n/(2*neg), 1: n/(2*pos)}\n",
    "\n",
    "CLASS_WEIGHTS = compute_class_weights(y_tr)\n",
    "print(\"Class weights:\", CLASS_WEIGHTS)\n",
    "\n",
    "# ========= 5) Build tf.data + TextVectorization pipeline (TEXT ONLY for now) =========\n",
    "print(\"Step 5: Building TextVectorizationâ€¦\")\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "MAX_TOKENS = 30000\n",
    "SEQ_LEN = 200\n",
    "HAS_GPU = bool(tf.config.list_physical_devices('GPU'))\n",
    "BATCH = 64 if HAS_GPU else 32\n",
    "EPOCHS = 16  # early stopping will halt earlier\n",
    "print(f\"  HAS_GPU={HAS_GPU} | BATCH={BATCH}\")\n",
    "\n",
    "# Raw tf.data datasets (text only)\n",
    "ds_tr_raw = tf.data.Dataset.from_tensor_slices((X_tr_text, y_tr))\n",
    "ds_va_raw = tf.data.Dataset.from_tensor_slices((X_va_text, y_va))\n",
    "ds_te_raw = tf.data.Dataset.from_tensor_slices(X_test_text)\n",
    "\n",
    "# Vectoriser (adapt on train only)\n",
    "text_vec = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQ_LEN,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\"\n",
    ")\n",
    "text_vec.adapt(ds_tr_raw.map(lambda x, y: x).batch(256))\n",
    "print(\"  TextVectorization adapted.\")\n",
    "\n",
    "# ========= 6) Load engineered features (or compute), then split with same indices =========\n",
    "print(\"Step 6: Loading / computing engineered featuresâ€¦\")\n",
    "TR_FEATS_PKL = \"data/processed/train_features.pkl\"\n",
    "TE_FEATS_PKL = \"data/processed/test_features.pkl\"\n",
    "\n",
    "def extract_features_v2(df, text_col):\n",
    "    # (Paste your 03_feature_engineering extractor here if not already imported)\n",
    "    import re\n",
    "    s = df[text_col].fillna(\"\").astype(str)\n",
    "    feats = pd.DataFrame(index=df.index)\n",
    "    _safe_div = lambda a,b: a/np.maximum(b,1)\n",
    "    NEG_WORDS = {\"ban\",\"banned\",\"remove\",\"removed\",\"delete\",\"deleted\",\"violation\",\"warn\",\"warning\",\"report\",\"flag\",\"hate\",\"toxic\",\"idiot\",\"stupid\",\"dumb\",\"trash\",\"nonsense\",\"shut\",\"shutup\",\"racist\",\"sexist\",\"harass\",\"abuse\",\"spam\",\"brigade\",\"rule\",\"rules\",\"automod\",\"mod\",\"moderator\"}\n",
    "    POS_WORDS = {\"please\",\"thanks\",\"thank\",\"appreciate\",\"sorry\",\"kindly\",\"cheers\"}\n",
    "    QUESTION_WORDS = {\"why\",\"how\",\"what\",\"when\",\"where\",\"which\",\"who\"}\n",
    "    NEGATIONS = {\"not\",\"no\",\"never\",\"n't\"}\n",
    "    EMOJI_RE = re.compile(r\"[\\U0001F300-\\U0001FAFF]\")\n",
    "    REPEAT_CHAR_RE = re.compile(r\"(.)\\1{2,}\")\n",
    "    MD_LINK_RE = re.compile(r\"\\[[^\\]]+\\]\\([^)]+\\)\")\n",
    "    def _count_tokens(text, vocab): \n",
    "        toks = text.lower().split(); \n",
    "        return sum(t in vocab for t in toks)\n",
    "    feats[\"char_count\"]  = s.str.len()\n",
    "    feats[\"word_count\"]  = s.str.split().str.len().astype(\"int64\")\n",
    "    feats[\"uniq_word_count\"] = s.apply(lambda x: len(set(x.lower().split())))\n",
    "    feats[\"lexical_diversity\"] = _safe_div(feats[\"uniq_word_count\"], feats[\"word_count\"])\n",
    "    feats[\"avg_word_len\"] = _safe_div(feats[\"char_count\"], feats[\"word_count\"])\n",
    "    feats[\"upper_count\"] = s.str.count(r\"[A-Z]\")\n",
    "    feats[\"caps_ratio\"]  = _safe_div(feats[\"upper_count\"], feats[\"char_count\"])\n",
    "    feats[\"all_caps_words\"] = s.str.count(r\"\\b[A-Z]{2,}\\b\")\n",
    "    feats[\"excl_count\"]  = s.str.count(\"!\")\n",
    "    feats[\"ques_count\"]  = s.str.count(r\"\\?\")\n",
    "    feats[\"dots_count\"]  = s.str.count(r\"\\.\")\n",
    "    feats[\"ellipsis_count\"] = s.str.count(r\"\\.\\.\\.\")\n",
    "    feats[\"multi_excl\"]  = s.str.count(r\"!!+\")\n",
    "    feats[\"multi_ques\"]  = s.str.count(r\"\\?\\?+\")\n",
    "    feats[\"mix_punct\"]   = s.str.count(r\"[!?]{2,}\")\n",
    "    feats[\"punct_ratio\"] = _safe_div(feats[\"excl_count\"]+feats[\"ques_count\"]+feats[\"dots_count\"], feats[\"char_count\"])\n",
    "    feats[\"repeat_char\"] = s.apply(lambda x: len(REPEAT_CHAR_RE.findall(x)))\n",
    "    feats[\"has_user_mention\"]      = s.str.contains(r\"u/\\w+\", case=False, regex=True).astype(\"int8\")\n",
    "    feats[\"has_subreddit_mention\"] = s.str.contains(r\"r/\\w+\", case=False, regex=True).astype(\"int8\")\n",
    "    feats[\"quote_count\"]           = s.str.count(r\"^>|\\n>\", flags=re.MULTILINE)\n",
    "    feats[\"code_ticks\"]            = s.str.count(r\"`\")\n",
    "    feats[\"md_links\"]              = s.apply(lambda x: len(MD_LINK_RE.findall(x))).astype(\"int16\")\n",
    "    feats[\"has_url\"]     = s.str.contains(r\"http[s]?://\", case=False, regex=True).astype(\"int8\")\n",
    "    feats[\"email_count\"] = s.str.count(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "    feats[\"digit_count\"] = s.str.count(r\"\\d\")\n",
    "    feats[\"num_ratio\"]   = _safe_div(feats[\"digit_count\"], feats[\"char_count\"])\n",
    "    feats[\"emoji_count\"] = s.apply(lambda x: len(EMOJI_RE.findall(x))).astype(\"int16\")\n",
    "    feats[\"neg_lex_count\"] = s.apply(lambda x: _count_tokens(x, NEG_WORDS)).astype(\"int16\")\n",
    "    feats[\"pos_lex_count\"] = s.apply(lambda x: _count_tokens(x, POS_WORDS)).astype(\"int16\")\n",
    "    feats[\"neg_lex_ratio\"] = _safe_div(feats[\"neg_lex_count\"], feats[\"word_count\"])\n",
    "    feats[\"pos_lex_ratio\"] = _safe_div(feats[\"pos_lex_count\"], feats[\"word_count\"])\n",
    "    feats[\"you_count\"] = s.str.count(r\"\\byou\\b\", flags=re.IGNORECASE)\n",
    "    feats[\"i_count\"]   = s.str.count(r\"\\bi\\b\",   flags=re.IGNORECASE)\n",
    "    feats[\"you_ratio\"] = _safe_div(feats[\"you_count\"], feats[\"word_count\"])\n",
    "    feats[\"i_ratio\"]   = _safe_div(feats[\"i_count\"],   feats[\"word_count\"])\n",
    "    feats[\"wh_q_count\"]  = s.apply(lambda x: _count_tokens(x, QUESTION_WORDS)).astype(\"int16\")\n",
    "    feats[\"negate_count\"]= s.apply(lambda x: _count_tokens(x, NEGATIONS)).astype(\"int16\")\n",
    "    feats[\"starts_with_quote\"] = s.str.match(r'^\\s*[\"\\']').astype(\"int8\")\n",
    "    feats[\"ends_with_q\"]       = s.str.endswith(\"?\").astype(\"int8\")\n",
    "    feats[\"ends_with_excl\"]    = s.str.endswith(\"!\").astype(\"int8\")\n",
    "    feats[\"excl_ratio\"] = _safe_div(feats[\"excl_count\"], feats[\"char_count\"])\n",
    "    feats[\"ques_ratio\"] = _safe_div(feats[\"ques_count\"], feats[\"char_count\"])\n",
    "    feats = feats.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    return feats\n",
    "\n",
    "if os.path.exists(TR_FEATS_PKL) and os.path.exists(TE_FEATS_PKL):\n",
    "    train_features = pd.read_pickle(TR_FEATS_PKL)\n",
    "    test_features  = pd.read_pickle(TE_FEATS_PKL)\n",
    "    print(\"  Loaded cached features.\")\n",
    "else:\n",
    "    train_features = extract_features_v2(train_df, TEXT_COL)\n",
    "    test_features  = extract_features_v2(test_df,  TEXT_COL)\n",
    "    os.makedirs(\"data/processed\", exist_ok=True)\n",
    "    train_features.to_pickle(TR_FEATS_PKL)\n",
    "    test_features.to_pickle(TE_FEATS_PKL)\n",
    "    print(\"  Computed and cached features.\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "feat_scaler = StandardScaler()\n",
    "feats_tr_all = feat_scaler.fit_transform(train_features.values.astype(np.float32))\n",
    "feats_te     = feat_scaler.transform(test_features.values.astype(np.float32))\n",
    "\n",
    "# Use *the same* split indices as for text\n",
    "X_idx = np.arange(len(X_text))\n",
    "idx_tr, idx_va, _, _ = train_test_split(X_idx, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "feats_tr_split = feats_tr_all[idx_tr]\n",
    "feats_va_split = feats_tr_all[idx_va]\n",
    "print(\"  Feats shapes:\", feats_tr_split.shape, feats_va_split.shape, feats_te.shape)\n",
    "\n",
    "# ========= 7) Build tf.data that yields (text, aux_feats) =========\n",
    "print(\"Step 7: Building tf.data with auxiliary featuresâ€¦\")\n",
    "def make_ds_with_feats(x_arr, feats_arr, y_arr=None, train=False):\n",
    "    if y_arr is None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((x_arr, feats_arr))\n",
    "        if train:\n",
    "            ds = ds.shuffle(len(x_arr), seed=SEED)\n",
    "        ds = ds.batch(BATCH).map(\n",
    "            lambda txt, f: (text_vec(txt), f),\n",
    "            num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((x_arr, feats_arr, y_arr))\n",
    "        if train:\n",
    "            ds = ds.shuffle(len(x_arr), seed=SEED)\n",
    "        ds = ds.batch(BATCH).map(\n",
    "            lambda txt, f, y: ((text_vec(txt), f), y),\n",
    "            num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "ds_tr = make_ds_with_feats(X_tr_text, feats_tr_split, y_tr, train=True)\n",
    "ds_va = make_ds_with_feats(X_va_text, feats_va_split, y_va, train=False)\n",
    "ds_te = make_ds_with_feats(X_test_text, feats_te, y_arr=None, train=False)\n",
    "print(\"  tf.data ready.\")\n",
    "\n",
    "# --- Sanity check: make sure the dataset yields one batch ---\n",
    "for batch in ds_tr.take(1):\n",
    "    (tok_ids, aux_feats), yb = batch\n",
    "    print(\"Sample batch:\", tok_ids.shape, aux_feats.shape, yb.shape)\n",
    "# Expect something like: (32 or 64, SEQ_LEN), (32 or 64, 42), (32 or 64,)\n",
    "\n",
    "\n",
    "# ========= 8) Build model (dual-branch text + aux feats) =========\n",
    "print(\"Step 8: Building modelâ€¦\")\n",
    "tf.random.set_seed(SEED)\n",
    "vocab_size   = MAX_TOKENS + 2\n",
    "embed_dim    = 128\n",
    "gru_units    = 96\n",
    "dropout_rate = 0.35\n",
    "l2_reg       = tf.keras.regularizers.l2(1e-5)\n",
    "\n",
    "tok_in = tf.keras.Input(shape=(SEQ_LEN,), dtype=\"int32\", name=\"tok_ids\")\n",
    "emb = tf.keras.layers.Embedding(vocab_size, embed_dim, mask_zero=False,\n",
    "                                embeddings_regularizer=l2_reg, name=\"emb\")(tok_in)\n",
    "\n",
    "# RNN branch\n",
    "rnn = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.GRU(gru_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.15),\n",
    "    name=\"bigru_seq\"\n",
    ")(emb)\n",
    "rnn = tf.keras.layers.GlobalMaxPooling1D(name=\"rnn_gmp\")(rnn)\n",
    "\n",
    "# CNN branch\n",
    "cnn = tf.keras.layers.SpatialDropout1D(0.2, name=\"spatial_dropout\")(emb)\n",
    "cnn = tf.keras.layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\", kernel_regularizer=l2_reg)(cnn)\n",
    "cnn = tf.keras.layers.Conv1D(128, 5, padding=\"same\", activation=\"relu\", kernel_regularizer=l2_reg)(cnn)\n",
    "gmp = tf.keras.layers.GlobalMaxPooling1D(name=\"cnn_gmp\")(cnn)\n",
    "gap = tf.keras.layers.GlobalAveragePooling1D(name=\"cnn_gap\")(cnn)\n",
    "cnn = tf.keras.layers.Concatenate(name=\"cnn_pool\")([gmp, gap])\n",
    "\n",
    "# Aux (engineered) features\n",
    "aux_in = tf.keras.Input(shape=(feats_tr_all.shape[1],), dtype=\"float32\", name=\"aux_feats\")\n",
    "aux_h  = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=l2_reg)(aux_in)\n",
    "aux_h  = tf.keras.layers.Dropout(0.2)(aux_h)\n",
    "\n",
    "# Fuse and head\n",
    "h = tf.keras.layers.Concatenate(name=\"fusion\")([rnn, cnn, aux_h])\n",
    "h = tf.keras.layers.Dropout(dropout_rate)(h)\n",
    "h = tf.keras.layers.Dense(192, activation=\"relu\", kernel_regularizer=l2_reg)(h)\n",
    "h = tf.keras.layers.Dropout(dropout_rate)(h)\n",
    "out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"out\")(h)\n",
    "\n",
    "model = tf.keras.Model(inputs=[tok_in, aux_in], outputs=out, name=\"hybrid_text_feats\")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[tf.keras.metrics.AUC(name=\"auc\"),\n",
    "             tf.keras.metrics.Precision(name=\"prec\"),\n",
    "             tf.keras.metrics.Recall(name=\"rec\")]\n",
    ")\n",
    "\n",
    "# --- Warm-up: force a single forward pass to compile kernels/graph ---\n",
    "print(\"Warm-up: building graph with a single batch â€¦\")\n",
    "_ = model.predict(ds_tr.take(1), verbose=0)\n",
    "print(\"Warm-up predict done. Starting trainingâ€¦\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ========= 9) Train with callbacks =========\n",
    "print(\"Step 9: Trainingâ€¦\")\n",
    "ckpt_dir = (KAGGLE_WORKING if IS_KAGGLE else \"models\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "ckpt_path = os.path.join(ckpt_dir, \"tf_neural_baseline_v2.keras\")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=5e-5, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_loss\", save_best_only=True, verbose=1)\n",
    "]\n",
    "history = model.fit(\n",
    "    ds_tr,\n",
    "    validation_data=ds_va,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1   # was 2\n",
    ")\n",
    "\n",
    "print(\"  Training done.\")\n",
    "\n",
    "# ========= 10) Threshold tuning & evaluation =========\n",
    "print(\"Step 10: Threshold tuningâ€¦\")\n",
    "va_probs = model.predict(ds_va, verbose=0).ravel()\n",
    "thr_grid = np.linspace(0.30, 0.70, 81)\n",
    "f1s = [f1_score(y_va, (va_probs >= t).astype(int), average=\"macro\") for t in thr_grid]\n",
    "best_idx = int(np.argmax(f1s))\n",
    "best_threshold = float(thr_grid[best_idx])\n",
    "val_f1_macro = float(f1s[best_idx])\n",
    "print(f\"  Best threshold = {best_threshold:.3f} | Val F1(macro) = {val_f1_macro:.4f}\")\n",
    "\n",
    "y_pred_va = (va_probs >= best_threshold).astype(int)\n",
    "print(\"Validation confusion matrix:\\n\", confusion_matrix(y_va, y_pred_va))\n",
    "print(classification_report(y_va, y_pred_va, digits=4))\n",
    "\n",
    "# ========= 11) Predict test & build submission =========\n",
    "print(\"Step 11: Predicting test & writing submissionâ€¦\")\n",
    "\n",
    "# 11a) Explicit tokenisation to arrays (avoids Dataset tuple quirks)\n",
    "X_te_tok = text_vec(tf.constant(X_test_text))\n",
    "# If you're low on memory, you can .numpy() this after prediction; Keras accepts EagerTensors.\n",
    "# X_te_tok_np = X_te_tok.numpy()\n",
    "\n",
    "# 11b) Predict with explicit two-input list [tokens, features]\n",
    "test_probs = model.predict([X_te_tok, feats_te], batch_size=BATCH, verbose=0).ravel()\n",
    "test_pred  = (test_probs >= best_threshold).astype(int)\n",
    "\n",
    "submission = sample.copy()\n",
    "submission[TARGET_OUT] = test_pred.astype(int)\n",
    "\n",
    "# Validation of format (unchanged)\n",
    "errors = []\n",
    "if list(submission.columns) != list(sample.columns):\n",
    "    errors.append(f\"Columns mismatch. Expected {list(sample.columns)}, got {list(submission.columns)}\")\n",
    "if len(submission) != len(sample):\n",
    "    errors.append(f\"Row count mismatch. Expected {len(sample)}, got {len(submission)}\")\n",
    "if not submission[ID_COL].equals(sample[ID_COL]):\n",
    "    if set(submission[ID_COL]) != set(sample[ID_COL]):\n",
    "        missing = list(sorted(set(sample[ID_COL]) - set(submission[ID_COL])))[:5]\n",
    "        extra   = list(sorted(set(submission[ID_COL]) - set(sample[ID_COL])))[:5]\n",
    "        errors.append(f\"ID set differs. Missing: {missing} | Extra: {extra}\")\n",
    "    else:\n",
    "        errors.append(\"ID order differs from sample. Must match sample_submission order.\")\n",
    "if submission[TARGET_OUT].isna().any():\n",
    "    errors.append(\"Target has NaNs.\")\n",
    "u = set(np.unique(submission[TARGET_OUT]))\n",
    "if not u.issubset({0,1}):\n",
    "    errors.append(f\"Target invalid values {sorted(u)}; must be 0/1.\")\n",
    "if errors:\n",
    "    print(\"âŒ Submission invalid:\")\n",
    "    for e in errors: print(\" -\", e)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    submission.to_csv(OUT_KAGGLE, index=False)\n",
    "    print(f\"âœ… Saved Kaggle file: {OUT_KAGGLE}\")\n",
    "submission.to_csv(OUT_LOCAL, index=False)\n",
    "print(f\"âœ… Saved local copy : {OUT_LOCAL}\")\n",
    "\n",
    "\n",
    "# ========= 12) Log run info =========\n",
    "print(\"Step 12: Logging run infoâ€¦\")\n",
    "run_info = {\n",
    "    \"task\": \"3.1_neural_baseline_hybrid\",\n",
    "    \"model\": \"Hybrid CNN+BiGRU + 42 feats\",\n",
    "    \"val_f1_macro\": float(val_f1_macro),\n",
    "    \"best_threshold\": float(best_threshold),\n",
    "    \"seed\": SEED,\n",
    "    \"params\": {\n",
    "        \"max_tokens\": MAX_TOKENS, \"seq_len\": SEQ_LEN, \"batch\": BATCH, \"epochs\": EPOCHS,\n",
    "        \"embed_dim\": embed_dim, \"gru_units\": gru_units,\n",
    "        \"dropout_rate\": dropout_rate, \"l2\": 1e-5\n",
    "    }\n",
    "}\n",
    "with open(\"results/run_05_neural_baseline.json\",\"w\") as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
