{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe791e31-ae60-43b7-a2e6-9fcf95cffaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\n",
      "NumPy : 1.26.4\n",
      "Pandas: 2.2.3\n",
      "Using XGBoost …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold (val) = 0.610 | Val F1(macro) = 0.7602\n",
      "Validation confusion matrix:\n",
      " [[167  33]\n",
      " [ 64 142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7229    0.8350    0.7749       200\n",
      "           1     0.8114    0.6893    0.7454       206\n",
      "\n",
      "    accuracy                         0.7611       406\n",
      "   macro avg     0.7672    0.7622    0.7602       406\n",
      "weighted avg     0.7678    0.7611    0.7600       406\n",
      "\n",
      "✅ Saved local copy : submissions/submission.csv\n",
      "\n",
      "Model used: XGBoost\n",
      "Validation F1 (macro): 0.7602 at threshold 0.610\n",
      "Final submission head:\n",
      "    row_id  rule_violation\n",
      "0    2029               0\n",
      "1    2030               0\n",
      "2    2031               1\n",
      "3    2032               1\n",
      "4    2033               1\n"
     ]
    }
   ],
   "source": [
    "# 04_advanced_models.ipynb — Jigsaw Agile Community Rules (XGBoost + submit)\n",
    "\n",
    "# Works locally and on Kaggle (Internet OFF). Produces /kaggle/working/submission.csv on Kaggle.\n",
    "\n",
    "# ========= 0) Imports & environment info =========\n",
    "\n",
    "import sys, os, glob, re, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"NumPy :\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "f_va = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Preferred: XGBoost with early stopping; Fallback: Logistic Regression\n",
    "use_xgb = True\n",
    "best_threshold = 0.5\n",
    "val_f1 = None\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"Using XGBoost …\")\n",
    "    xgb_params = dict(\n",
    "        max_depth=8,\n",
    "        learning_rate=0.07,\n",
    "        n_estimators=800,              # large cap; early stopping will trim\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.9,\n",
    "        min_child_weight=1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    # Probabilities and dynamic threshold for F1(macro)\n",
    "    va_prob = model.predict_proba(X_va)[:, 1]\n",
    "    # Scan thresholds to maximise macro-F1 on validation\n",
    "    thr_grid = np.linspace(0.2, 0.8, 61)  # coarse but fine for small set\n",
    "    f1s = []\n",
    "    for t in thr_grid:\n",
    "        f1s.append(f1_score(y_va, (va_prob >= t).astype(int), average=\"macro\"))\n",
    "    best_idx = int(np.argmax(f1s))\n",
    "    best_threshold = float(thr_grid[best_idx])\n",
    "    val_f1 = float(f1s[best_idx])\n",
    "    print(f\"Best threshold (val) = {best_threshold:.3f} | Val F1(macro) = {val_f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix at best threshold\n",
    "    y_pred_va = (va_prob >= best_threshold).astype(int)\n",
    "    print(\"Validation confusion matrix:\\n\", confusion_matrix(y_va, y_pred_va))\n",
    "    print(classification_report(y_va, y_pred_va, digits=4))\n",
    "\n",
    "    # Refit on ALL data with best n_estimators (best_iteration_) if available\n",
    "    best_n = getattr(model, \"best_iteration\", None)\n",
    "    if best_n is None:\n",
    "        best_n = getattr(model, \"best_ntree_limit\", None)\n",
    "    if best_n is None:\n",
    "        best_n = xgb_params[\"n_estimators\"]\n",
    "    else:\n",
    "        best_n = int(best_n) + 1\n",
    "\n",
    "    model_final = xgb.XGBClassifier(**{**xgb_params, \"n_estimators\": best_n})\n",
    "    model_final.fit(X, y, verbose=False)\n",
    "\n",
    "    # Predict test with tuned threshold\n",
    "    test_prob = model_final.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_prob >= best_threshold).astype(int)\n",
    "\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"XGBoost unavailable or errored ({e}). Falling back to Logistic Regression.\")\n",
    "    use_xgb = False\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=3000,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    lr.fit(X_tr, y_tr)\n",
    "    va_prob = lr.predict_proba(X_va)[:, 1]\n",
    "    # threshold tuning\n",
    "    thr_grid = np.linspace(0.2, 0.8, 61)\n",
    "    f1s = [f1_score(y_va, (va_prob >= t).astype(int), average=\"macro\") for t in thr_grid]\n",
    "    best_idx = int(np.argmax(f1s))\n",
    "    best_threshold = float(thr_grid[best_idx])\n",
    "    val_f1 = float(f1s[best_idx])\n",
    "    print(f\"[LR] Best threshold (val) = {best_threshold:.3f} | Val F1(macro) = {val_f1:.4f}\")\n",
    "\n",
    "    y_pred_va = (va_prob >= best_threshold).astype(int)\n",
    "    print(\"Validation confusion matrix:\\n\", confusion_matrix(y_va, y_pred_va))\n",
    "    print(classification_report(y_va, y_pred_va, digits=4))\n",
    "\n",
    "    # Train on all & predict test\n",
    "    lr.fit(X, y)\n",
    "    test_prob = lr.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_prob >= best_threshold).astype(int)\n",
    "\n",
    "# ========= 6) Build & validate submission =========\n",
    "submission = sample.copy()\n",
    "submission[TARGET_OUT] = test_pred.astype(int)\n",
    "\n",
    "errors = []\n",
    "if list(submission.columns) != list(sample.columns):\n",
    "    errors.append(f\"Columns mismatch. Expected {list(sample.columns)}, got {list(submission.columns)}\")\n",
    "if len(submission) != len(sample):\n",
    "    errors.append(f\"Row count mismatch. Expected {len(sample)}, got {len(submission)}\")\n",
    "if not submission[ID_COL].equals(sample[ID_COL]):\n",
    "    if set(submission[ID_COL]) != set(sample[ID_COL]):\n",
    "        missing = list(sorted(set(sample[ID_COL]) - set(submission[ID_COL])))[:5]\n",
    "        extra   = list(sorted(set(submission[ID_COL]) - set(sample[ID_COL])))[:5]\n",
    "        errors.append(f\"ID set differs. Missing: {missing} | Extra: {extra}\")\n",
    "    else:\n",
    "        errors.append(\"ID order differs from sample. Must match sample_submission order.\")\n",
    "if submission[TARGET_OUT].isna().any():\n",
    "    errors.append(\"Target has NaNs.\")\n",
    "u = set(np.unique(submission[TARGET_OUT]))\n",
    "if not u.issubset({0,1}):\n",
    "    errors.append(f\"Target invalid values {sorted(u)}; must be 0/1.\")\n",
    "\n",
    "if errors:\n",
    "    print(\"❌ Submission invalid:\")\n",
    "    for e in errors: print(\" -\", e)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# ========= 7) Save submission (Kaggle + local) =========\n",
    "if IS_KAGGLE:\n",
    "    submission.to_csv(OUT_KAGGLE, index=False)\n",
    "    print(f\"✅ Saved Kaggle file: {OUT_KAGGLE}\")\n",
    "submission.to_csv(OUT_LOCAL, index=False)\n",
    "print(f\"✅ Saved local copy : {OUT_LOCAL}\")\n",
    "\n",
    "print(f\"\\nModel used: {'XGBoost' if use_xgb else 'LogisticRegression'}\")\n",
    "print(f\"Validation F1 (macro): {val_f1:.4f} at threshold {best_threshold:.3f}\")\n",
    "print(\"Final submission head:\\n\", submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38177072-36e2-43ed-bbdf-ccb675b1c0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
