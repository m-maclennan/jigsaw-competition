{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34509c0f-432b-464e-9263-89ee21c36423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 08:03:38) [Clang 14.0.6 ]\n",
      "NumPy : 1.26.4\n",
      "Pandas: 2.2.3\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not locate train/test/sample_submission CSVs.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m SAMP_PATH  = _first_existing([\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mKAGGLE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/sample_submission.csv\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdata/raw/sample_submission.csv\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (TRAIN_PATH \u001b[38;5;129;01mand\u001b[39;00m TEST_PATH \u001b[38;5;129;01mand\u001b[39;00m SAMP_PATH):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCould not locate train/test/sample_submission CSVs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìÑ Loading train.csv from:\u001b[39m\u001b[33m\"\u001b[39m, TRAIN_PATH)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìÑ Loading test.csv from :\u001b[39m\u001b[33m\"\u001b[39m, TEST_PATH)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Could not locate train/test/sample_submission CSVs."
     ]
    }
   ],
   "source": [
    "# 05_ensemble.ipynb ‚Äî Jigsaw Agile Community Rules\n",
    "# Purpose:\n",
    "#  - Load OOF (train) & test probabilities from prior notebooks\n",
    "#  - Explore blends: mean, rank-avg, weight-avg (optional weights from results/models.json)\n",
    "#  - Train a simple meta-learner (LogReg stacker) as Platt-like calibrator\n",
    "#  - Tune threshold (macro-F1) on OOF\n",
    "#  - Produce /kaggle/working/submission.csv and submissions/submission.csv\n",
    "\n",
    "# ========= 0) Imports & environment =========\n",
    "import os, sys, glob, json, math, warnings, re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"NumPy :\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "KAGGLE_DIR = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "KAGGLE_WORKING = \"/kaggle/working\" if IS_KAGGLE else None\n",
    "OUT_KAGGLE = os.path.join(KAGGLE_WORKING, \"submission.csv\") if IS_KAGGLE else None\n",
    "\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "OUT_LOCAL = \"submissions/submission.csv\"\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# ========= 1) Load core CSVs (Kaggle-first then local) =========\n",
    "def _first_existing(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "TRAIN_PATH = _first_existing([f\"{KAGGLE_DIR}/train.csv\", \"data/raw/train.csv\"])\n",
    "TEST_PATH  = _first_existing([f\"{KAGGLE_DIR}/test.csv\",  \"data/raw/test.csv\"])\n",
    "SAMP_PATH  = _first_existing([f\"{KAGGLE_DIR}/sample_submission.csv\", \"data/raw/sample_submission.csv\"])\n",
    "\n",
    "if not (TRAIN_PATH and TEST_PATH and SAMP_PATH):\n",
    "    raise FileNotFoundError(\"Could not locate train/test/sample_submission CSVs.\")\n",
    "\n",
    "print(\"üìÑ Loading train.csv from:\", TRAIN_PATH)\n",
    "print(\"üìÑ Loading test.csv from :\", TEST_PATH)\n",
    "print(\"üìÑ Loading sample_submission.csv from:\", SAMP_PATH)\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "sample   = pd.read_csv(SAMP_PATH)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test  shape:\", test_df.shape)\n",
    "print(\"Sample shape:\", sample.shape)\n",
    "\n",
    "TEXT_COL = next((c for c in [\"comment_text\",\"body\",\"text\"] if c in train_df.columns), None)\n",
    "TARGET_COL = next((c for c in [\"rule_violation\",\"target\",\"label\"] if c in train_df.columns), None)\n",
    "ID_COL, TARGET_OUT = sample.columns[0], sample.columns[1]\n",
    "assert TARGET_COL, \"Target column not found.\"\n",
    "\n",
    "# ========= 2) Load OOF & test probabilities =========\n",
    "OOF_DIR  = \"results/oof\"\n",
    "TEST_DIR = \"results/test_probs\"\n",
    "oof_files  = sorted(glob.glob(os.path.join(OOF_DIR, \"*_oof.csv\")))\n",
    "test_files = sorted(glob.glob(os.path.join(TEST_DIR, \"*_test.csv\")))\n",
    "\n",
    "if len(oof_files) == 0 or len(test_files) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No OOF/test files found.\\n\"\n",
    "        f\"Expected OOF in {OOF_DIR}/*_oof.csv and test in {TEST_DIR}/*_test.csv \"\n",
    "        f\"with columns: row_id, prob\"\n",
    "    )\n",
    "\n",
    "def model_key_from_path(p):\n",
    "    base = os.path.basename(p)\n",
    "    # strip suffixes\n",
    "    return re.sub(r\"(_oof|_test)?\\.csv$\", \"\", base)\n",
    "\n",
    "# Map model key -> file path\n",
    "oof_map  = {model_key_from_path(p).replace(\"_oof\",\"\"): p for p in oof_files}\n",
    "test_map = {model_key_from_path(p).replace(\"_test\",\"\"): p for p in test_files}\n",
    "\n",
    "# Keep intersection only (models that have BOTH oof and test files)\n",
    "models = sorted(set(oof_map).intersection(set(test_map)))\n",
    "if len(models) < 2:\n",
    "    raise RuntimeError(f\"Need at least 2 models with both OOF and test probs; found: {models}\")\n",
    "\n",
    "print(\"\\nModels detected:\")\n",
    "for m in models:\n",
    "    print(\" -\", m)\n",
    "\n",
    "# Load & merge OOF by row_id aligned to train order\n",
    "train_ids = train_df[ID_COL].values\n",
    "y_true = train_df[TARGET_COL].astype(int).values\n",
    "\n",
    "oof_df = pd.DataFrame({ID_COL: train_ids})\n",
    "for m in models:\n",
    "    tmp = pd.read_csv(oof_map[m])\n",
    "    assert {\"row_id\",\"prob\"}.issubset(tmp.columns), f\"OOF file {oof_map[m]} missing columns.\"\n",
    "    oof_df = oof_df.merge(tmp.rename(columns={\"row_id\":ID_COL, \"prob\":f\"prob_{m}\"}), on=ID_COL, how=\"left\")\n",
    "\n",
    "missing_cols = [c for c in oof_df.columns if c.startswith(\"prob_\") and oof_df[c].isna().any()]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"OOF merge mismatch; NaNs in {missing_cols}. Ensure row_id alignment with train.csv\")\n",
    "\n",
    "# Load & merge Test by row_id aligned to sample order\n",
    "test_ids = sample[ID_COL].values\n",
    "test_df_probs = pd.DataFrame({ID_COL: test_ids})\n",
    "for m in models:\n",
    "    tmp = pd.read_csv(test_map[m])\n",
    "    assert {\"row_id\",\"prob\"}.issubset(tmp.columns), f\"Test file {test_map[m]} missing columns.\"\n",
    "    test_df_probs = test_df_probs.merge(tmp.rename(columns={\"row_id\":ID_COL, \"prob\":f\"prob_{m}\"}), on=ID_COL, how=\"left\")\n",
    "\n",
    "missing_cols_t = [c for c in test_df_probs.columns if c.startswith(\"prob_\") and test_df_probs[c].isna().any()]\n",
    "if missing_cols_t:\n",
    "    raise ValueError(f\"TEST merge mismatch; NaNs in {missing_cols_t}. Ensure row_id alignment with sample_submission.csv\")\n",
    "\n",
    "print(\"\\nOOF matrix shape:\", oof_df.shape, \"| Test matrix shape:\", test_df_probs.shape)\n",
    "\n",
    "# ========= 3) Quick diagnostics =========\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "probs_mat = oof_df[[c for c in oof_df.columns if c.startswith(\"prob_\")]].values\n",
    "corr = np.corrcoef(probs_mat, rowvar=False)\n",
    "print(\"\\nModel correlation matrix (OOF probs):\")\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(corr)\n",
    "\n",
    "# ========= 4) Blending strategies =========\n",
    "def macro_f1_at_threshold(y, p, thr):\n",
    "    pred = (p >= thr).astype(int)\n",
    "    return f1_score(y, pred, average=\"macro\")\n",
    "\n",
    "def tune_threshold(y, p, grid=None):\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.30, 0.70, 81)\n",
    "    f1s = [macro_f1_at_threshold(y, p, t) for t in grid]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(grid[i]), float(f1s[i])\n",
    "\n",
    "# 4a) Simple mean\n",
    "mean_oof = probs_mat.mean(axis=1)\n",
    "thr_mean, f1_mean = tune_threshold(y_true, mean_oof)\n",
    "print(f\"\\n[Mean] OOF macro-F1 = {f1_mean:.4f} @ thr={thr_mean:.3f}\")\n",
    "\n",
    "# 4b) Rank average (robust when scales differ)\n",
    "rank_oof = np.mean(np.argsort(np.argsort(probs_mat, axis=0), axis=0), axis=1) / (len(y_true)-1)\n",
    "thr_rank, f1_rank = tune_threshold(y_true, rank_oof)\n",
    "print(f\"[Rank-Avg] OOF macro-F1 = {f1_rank:.4f} @ thr={thr_rank:.3f}\")\n",
    "\n",
    "# 4c) Weighted average\n",
    "#    - If results/models.json exists and has val_f1 per model, use them as soft weights; else uniform.\n",
    "weights = None\n",
    "meta_path = \"results/models.json\"\n",
    "if os.path.exists(meta_path):\n",
    "    try:\n",
    "        meta = json.load(open(meta_path))\n",
    "        # meta can be list of dicts or dict keyed by model\n",
    "        valf1_by_model = {}\n",
    "        if isinstance(meta, list):\n",
    "            for r in meta:\n",
    "                if \"model\" in r and \"val_f1\" in r: valf1_by_model[r[\"model\"]] = r[\"val_f1\"]\n",
    "        elif isinstance(meta, dict):\n",
    "            for k,v in meta.items():\n",
    "                if isinstance(v, dict) and \"val_f1\" in v: valf1_by_model[k] = v[\"val_f1\"]\n",
    "        w = []\n",
    "        for m in models:\n",
    "            w.append(max(1e-6, float(valf1_by_model.get(m, 1.0))))\n",
    "        weights = np.array(w, dtype=float)\n",
    "        weights = weights / weights.sum()\n",
    "        print(\"\\nUsing val_f1-based weights:\", dict(zip(models, np.round(weights,4))))\n",
    "    except Exception as e:\n",
    "        print(\"Could not parse results/models.json; falling back to uniform weights.\", e)\n",
    "\n",
    "if weights is None:\n",
    "    weights = np.ones(len(models), dtype=float) / len(models)\n",
    "\n",
    "wavg_oof = np.average(probs_mat, axis=1, weights=weights)\n",
    "thr_wavg, f1_wavg = tune_threshold(y_true, wavg_oof)\n",
    "print(f\"[Weighted-Avg] OOF macro-F1 = {f1_wavg:.4f} @ thr={thr_wavg:.3f}\")\n",
    "\n",
    "# 4d) Logistic Regression stacker (acts as Platt-style calibrator too)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X_oof = probs_mat\n",
    "y_oof = y_true\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validated meta-probs to avoid bias\n",
    "meta_oof = np.zeros_like(y_oof, dtype=float)\n",
    "coefs = []\n",
    "for tr, va in skf.split(X_oof, y_oof):\n",
    "    clf = LogisticRegression(\n",
    "        solver=\"lbfgs\", max_iter=200, C=1.0, class_weight=\"balanced\", n_jobs=None\n",
    "    )\n",
    "    clf.fit(X_oof[tr], y_oof[tr])\n",
    "    meta_oof[va] = clf.predict_proba(X_oof[va])[:,1]\n",
    "    coefs.append(clf.coef_.ravel())\n",
    "\n",
    "thr_meta, f1_meta = tune_threshold(y_oof, meta_oof)\n",
    "coef_mean = np.mean(coefs, axis=0)\n",
    "print(f\"[LR Stacker] OOF macro-F1 = {f1_meta:.4f} @ thr={thr_meta:.3f}\")\n",
    "print(\" Meta weights (mean coef):\", dict(zip(models, np.round(coef_mean,4))))\n",
    "\n",
    "# ========= 5) Pick the winner on OOF =========\n",
    "candidates = [\n",
    "    (\"mean\", f1_mean, thr_mean, mean_oof),\n",
    "    (\"rank\", f1_rank, thr_rank, rank_oof),\n",
    "    (\"wavg\", f1_wavg, thr_wavg, wavg_oof),\n",
    "    (\"stacker\", f1_meta, thr_meta, meta_oof),\n",
    "]\n",
    "winner = sorted(candidates, key=lambda x: x[1], reverse=True)[0]\n",
    "WIN_NAME, WIN_F1, WIN_THR, WIN_OOF = winner\n",
    "print(f\"\\n=== WINNER: {WIN_NAME} | OOF macro-F1={WIN_F1:.4f} @ thr={WIN_THR:.3f} ===\")\n",
    "\n",
    "# ========= 6) Build test predictions in the same way =========\n",
    "test_mat = test_df_probs[[c for c in test_df_probs.columns if c.startswith(\"prob_\")]].values\n",
    "\n",
    "if WIN_NAME == \"mean\":\n",
    "    test_probs = test_mat.mean(axis=1)\n",
    "elif WIN_NAME == \"rank\":\n",
    "    test_probs = np.mean(np.argsort(np.argsort(test_mat, axis=0), axis=0), axis=1) / (len(test_mat)-1)\n",
    "elif WIN_NAME == \"wavg\":\n",
    "    test_probs = np.average(test_mat, axis=1, weights=weights)\n",
    "else:\n",
    "    # Fit LR on full OOF to get final stacker, then apply to test\n",
    "    clf_full = LogisticRegression(solver=\"lbfgs\", max_iter=200, C=1.0, class_weight=\"balanced\")\n",
    "    clf_full.fit(X_oof, y_oof)\n",
    "    test_probs = clf_full.predict_proba(test_mat)[:,1]\n",
    "\n",
    "test_pred = (test_probs >= WIN_THR).astype(int)\n",
    "\n",
    "# ========= 7) Validate and write submission =========\n",
    "submission = sample.copy()\n",
    "submission[TARGET_OUT] = test_pred.astype(int)\n",
    "\n",
    "errors = []\n",
    "if list(submission.columns) != list(sample.columns):\n",
    "    errors.append(f\"Columns mismatch. Expected {list(sample.columns)}, got {list(submission.columns)}\")\n",
    "if len(submission) != len(sample):\n",
    "    errors.append(f\"Row count mismatch. Expected {len(sample)}, got {len(submission)}\")\n",
    "if not submission[ID_COL].equals(sample[ID_COL]):\n",
    "    if set(submission[ID_COL]) != set(sample[ID_COL]):\n",
    "        missing = list(sorted(set(sample[ID_COL]) - set(submission[ID_COL])))[:5]\n",
    "        extra   = list(sorted(set(submission[ID_COL]) - set(sample[ID_COL])))[:5]\n",
    "        errors.append(f\"ID set differs. Missing: {missing} | Extra: {extra}\")\n",
    "    else:\n",
    "        errors.append(\"ID order differs from sample. Must match sample_submission order.\")\n",
    "if submission[TARGET_OUT].isna().any():\n",
    "    errors.append(\"Target has NaNs.\")\n",
    "u = set(np.unique(submission[TARGET_OUT]))\n",
    "if not u.issubset({0,1}):\n",
    "    errors.append(f\"Target invalid values {sorted(u)}; must be 0/1.\")\n",
    "if errors:\n",
    "    print(\"‚ùå Submission invalid:\"); [print(\" -\", e) for e in errors]; raise SystemExit(1)\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    submission.to_csv(OUT_KAGGLE, index=False)\n",
    "    print(f\"‚úÖ Saved Kaggle file: {OUT_KAGGLE}\")\n",
    "submission.to_csv(OUT_LOCAL, index=False)\n",
    "print(f\"‚úÖ Saved local copy : {OUT_LOCAL}\")\n",
    "\n",
    "# ========= 8) Log run info =========\n",
    "run_info = {\n",
    "    \"task\": \"05_ensemble\",\n",
    "    \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"models\": models,\n",
    "    \"winner\": WIN_NAME,\n",
    "    \"oof_f1_macro\": float(WIN_F1),\n",
    "    \"threshold\": float(WIN_THR),\n",
    "}\n",
    "with open(\"results/run_05_ensemble.json\",\"w\") as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "\n",
    "print(\"\\nFinal summary:\")\n",
    "print(f\" Winner: {WIN_NAME} | OOF F1(macro)={WIN_F1:.4f} | thr={WIN_THR:.3f}\")\n",
    "print(\" First 5 submission rows:\\n\", submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edf868-648e-4e1f-9f55-871303e8fc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac-py312)",
   "language": "python",
   "name": "tf-mac-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
