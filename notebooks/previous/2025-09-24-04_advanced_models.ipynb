{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe791e31-ae60-43b7-a2e6-9fcf95cffaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\n",
      "NumPy : 1.26.4\n",
      "Pandas: 2.2.3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumPy :\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pd\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m---> 14\u001b[0m f_va \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     15\u001b[0m     X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Preferred: XGBoost with early stopping; Fallback: Logistic Regression\u001b[39;00m\n\u001b[1;32m     19\u001b[0m use_xgb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# 04_advanced_models.ipynb — Jigsaw Agile Community Rules (XGBoost + submit)\n",
    "\n",
    "# Works locally and on Kaggle (Internet OFF). Produces /kaggle/working/submission.csv on Kaggle.\n",
    "\n",
    "# ========= 0) Imports & environment info =========\n",
    "\n",
    "import sys, os, glob, re, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"NumPy :\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "f_va = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Preferred: XGBoost with early stopping; Fallback: Logistic Regression\n",
    "use_xgb = True\n",
    "best_threshold = 0.5\n",
    "val_f1 = None\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"Using XGBoost …\")\n",
    "    xgb_params = dict(\n",
    "        max_depth=8,\n",
    "        learning_rate=0.07,\n",
    "        n_estimators=800,              # large cap; early stopping will trim\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.9,\n",
    "        min_child_weight=1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    # Probabilities and dynamic threshold for F1(macro)\n",
    "    va_prob = model.predict_proba(X_va)[:, 1]\n",
    "    # Scan thresholds to maximise macro-F1 on validation\n",
    "    thr_grid = np.linspace(0.2, 0.8, 61)  # coarse but fine for small set\n",
    "    f1s = []\n",
    "    for t in thr_grid:\n",
    "        f1s.append(f1_score(y_va, (va_prob >= t).astype(int), average=\"macro\"))\n",
    "    best_idx = int(np.argmax(f1s))\n",
    "    best_threshold = float(thr_grid[best_idx])\n",
    "    val_f1 = float(f1s[best_idx])\n",
    "    print(f\"Best threshold (val) = {best_threshold:.3f} | Val F1(macro) = {val_f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix at best threshold\n",
    "    y_pred_va = (va_prob >= best_threshold).astype(int)\n",
    "    print(\"Validation confusion matrix:\\n\", confusion_matrix(y_va, y_pred_va))\n",
    "    print(classification_report(y_va, y_pred_va, digits=4))\n",
    "\n",
    "    # Refit on ALL data with best n_estimators (best_iteration_) if available\n",
    "    best_n = getattr(model, \"best_iteration\", None)\n",
    "    if best_n is None:\n",
    "        best_n = getattr(model, \"best_ntree_limit\", None)\n",
    "    if best_n is None:\n",
    "        best_n = xgb_params[\"n_estimators\"]\n",
    "    else:\n",
    "        best_n = int(best_n) + 1\n",
    "\n",
    "    model_final = xgb.XGBClassifier(**{**xgb_params, \"n_estimators\": best_n})\n",
    "    model_final.fit(X, y, verbose=False)\n",
    "\n",
    "    # Predict test with tuned threshold\n",
    "    test_prob = model_final.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_prob >= best_threshold).astype(int)\n",
    "\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"XGBoost unavailable or errored ({e}). Falling back to Logistic Regression.\")\n",
    "    use_xgb = False\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=3000,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    lr.fit(X_tr, y_tr)\n",
    "    va_prob = lr.predict_proba(X_va)[:, 1]\n",
    "    # threshold tuning\n",
    "    thr_grid = np.linspace(0.2, 0.8, 61)\n",
    "    f1s = [f1_score(y_va, (va_prob >= t).astype(int), average=\"macro\") for t in thr_grid]\n",
    "    best_idx = int(np.argmax(f1s))\n",
    "    best_threshold = float(thr_grid[best_idx])\n",
    "    val_f1 = float(f1s[best_idx])\n",
    "    print(f\"[LR] Best threshold (val) = {best_threshold:.3f} | Val F1(macro) = {val_f1:.4f}\")\n",
    "\n",
    "    y_pred_va = (va_prob >= best_threshold).astype(int)\n",
    "    print(\"Validation confusion matrix:\\n\", confusion_matrix(y_va, y_pred_va))\n",
    "    print(classification_report(y_va, y_pred_va, digits=4))\n",
    "\n",
    "    # Train on all & predict test\n",
    "    lr.fit(X, y)\n",
    "    test_prob = lr.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_prob >= best_threshold).astype(int)\n",
    "\n",
    "# === OOF + test probabilities for ensembling (XGBoost) ===\n",
    "import numpy as np, os, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "os.makedirs(\"results/oof\", exist_ok=True)\n",
    "os.makedirs(\"results/test_probs\", exist_ok=True)\n",
    "\n",
    "ID_COL = sample.columns[0]        # assumes you already loaded sample_submission.csv\n",
    "train_ids = train_df[ID_COL].values\n",
    "test_ids  = sample[ID_COL].values\n",
    "\n",
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_prob = np.zeros(len(train_df), dtype=float)\n",
    "test_prob_folds = []\n",
    "\n",
    "for fold,(tr,va) in enumerate(skf.split(X_combined, y), 1):\n",
    "    print(f\"[XGB OOF] Fold {fold}/{N_FOLDS}\")\n",
    "    model = xgb.XGBClassifier(**params)  # reuse your 'params'\n",
    "    model.fit(X_combined[tr], y[tr], eval_set=[(X_combined[va], y[va])],\n",
    "              verbose=False, early_stopping_rounds=50)\n",
    "    oof_prob[va] = model.predict_proba(X_combined[va])[:,1]\n",
    "    test_prob_folds.append(model.predict_proba(X_test)[:,1])\n",
    "\n",
    "test_prob = np.mean(np.column_stack(test_prob_folds), axis=1)\n",
    "\n",
    "# Save files for ensembling\n",
    "pd.DataFrame({\"row_id\": train_ids, \"prob\": oof_prob}).to_csv(\"results/oof/xgb_tfidf_feats_oof.csv\", index=False)\n",
    "pd.DataFrame({\"row_id\": test_ids,  \"prob\": test_prob}).to_csv(\"results/test_probs/xgb_tfidf_feats_test.csv\", index=False)\n",
    "print(\"Saved OOF/test probs for XGB → results/oof/xgb_tfidf_feats_oof.csv & results/test_probs/xgb_tfidf_feats_test.csv\")\n",
    "\n",
    "\n",
    "# ========= 6) Build & validate submission =========\n",
    "submission = sample.copy()\n",
    "submission[TARGET_OUT] = test_pred.astype(int)\n",
    "\n",
    "errors = []\n",
    "if list(submission.columns) != list(sample.columns):\n",
    "    errors.append(f\"Columns mismatch. Expected {list(sample.columns)}, got {list(submission.columns)}\")\n",
    "if len(submission) != len(sample):\n",
    "    errors.append(f\"Row count mismatch. Expected {len(sample)}, got {len(submission)}\")\n",
    "if not submission[ID_COL].equals(sample[ID_COL]):\n",
    "    if set(submission[ID_COL]) != set(sample[ID_COL]):\n",
    "        missing = list(sorted(set(sample[ID_COL]) - set(submission[ID_COL])))[:5]\n",
    "        extra   = list(sorted(set(submission[ID_COL]) - set(sample[ID_COL])))[:5]\n",
    "        errors.append(f\"ID set differs. Missing: {missing} | Extra: {extra}\")\n",
    "    else:\n",
    "        errors.append(\"ID order differs from sample. Must match sample_submission order.\")\n",
    "if submission[TARGET_OUT].isna().any():\n",
    "    errors.append(\"Target has NaNs.\")\n",
    "u = set(np.unique(submission[TARGET_OUT]))\n",
    "if not u.issubset({0,1}):\n",
    "    errors.append(f\"Target invalid values {sorted(u)}; must be 0/1.\")\n",
    "\n",
    "if errors:\n",
    "    print(\"❌ Submission invalid:\")\n",
    "    for e in errors: print(\" -\", e)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# ========= 7) Save submission (Kaggle + local) =========\n",
    "if IS_KAGGLE:\n",
    "    submission.to_csv(OUT_KAGGLE, index=False)\n",
    "    print(f\"✅ Saved Kaggle file: {OUT_KAGGLE}\")\n",
    "submission.to_csv(OUT_LOCAL, index=False)\n",
    "print(f\"✅ Saved local copy : {OUT_LOCAL}\")\n",
    "\n",
    "print(f\"\\nModel used: {'XGBoost' if use_xgb else 'LogisticRegression'}\")\n",
    "print(f\"Validation F1 (macro): {val_f1:.4f} at threshold {best_threshold:.3f}\")\n",
    "print(\"Final submission head:\\n\", submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38177072-36e2-43ed-bbdf-ccb675b1c0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
