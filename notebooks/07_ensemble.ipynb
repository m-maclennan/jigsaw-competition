{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8263c432-be46-4f45-8d72-0b14906d5487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Using:\n",
      "  train.csv → /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/train.csv\n",
      "  test.csv  → /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/test.csv\n",
      "  sample    → /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/sample_submission.csv\n",
      "[NAMES] ID_COL=row_id | TARGET_OUT=rule_violation\n",
      "[PATHS] Forced paths set and resolver overridden.\n"
     ]
    }
   ],
   "source": [
    "# ===== FORCE DATA PATHS (idempotent, no dependencies) =====\n",
    "import os, pandas as pd\n",
    "\n",
    "# 1) Point to your confirmed folder\n",
    "ABS_DIR = \"/Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw\"\n",
    "\n",
    "# 2) Build absolute paths (don't rely on any earlier vars)\n",
    "FORCE_TRAIN = os.path.join(ABS_DIR, \"train.csv\")\n",
    "FORCE_TEST  = os.path.join(ABS_DIR, \"test.csv\")\n",
    "FORCE_SAMP  = os.path.join(ABS_DIR, \"sample_submission.csv\")\n",
    "\n",
    "for p in (FORCE_TRAIN, FORCE_TEST, FORCE_SAMP):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Missing file: {p}\")\n",
    "\n",
    "print(\"📄 Using:\")\n",
    "print(\"  train.csv →\", FORCE_TRAIN)\n",
    "print(\"  test.csv  →\", FORCE_TEST)\n",
    "print(\"  sample    →\", FORCE_SAMP)\n",
    "\n",
    "# 3) Load once here and expose as globals\n",
    "train_df = pd.read_csv(FORCE_TRAIN)\n",
    "test_df  = pd.read_csv(FORCE_TEST)\n",
    "sample   = pd.read_csv(FORCE_SAMP)\n",
    "\n",
    "# 4) Standard names\n",
    "id_candidates = [c for c in sample.columns if (\"id\" in c.lower() or \"row\" in c.lower())]\n",
    "ID_COL = id_candidates[0] if id_candidates else sample.columns[0]\n",
    "TARGET_OUT = sample.columns[1] if sample.shape[1] >= 2 else \"prediction\"\n",
    "print(f\"[NAMES] ID_COL={ID_COL} | TARGET_OUT={TARGET_OUT}\")\n",
    "\n",
    "# 5) Override any later resolver functions/vars that older cells might call\n",
    "KAGGLE_DIR = \"\"  # neutralise Kaggle-only path\n",
    "def _first_existing(paths):\n",
    "    \"\"\"Always return our forced paths for the three dataset files.\"\"\"\n",
    "    prefer = {\n",
    "        \"train.csv\": FORCE_TRAIN,\n",
    "        \"test.csv\": FORCE_TEST,\n",
    "        \"sample_submission.csv\": FORCE_SAMP,\n",
    "    }\n",
    "    for p in paths:\n",
    "        base = os.path.basename(p)\n",
    "        if base in prefer:\n",
    "            return prefer[base]\n",
    "        if os.path.isabs(p) and os.path.exists(p):\n",
    "            return p\n",
    "    # Last resort\n",
    "    for v in prefer.values():\n",
    "        if os.path.exists(v):\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "# Also set the legacy names some cells expect\n",
    "TRAIN_PATH = FORCE_TRAIN\n",
    "TEST_PATH  = FORCE_TEST\n",
    "SAMP_PATH  = FORCE_SAMP\n",
    "\n",
    "DATA_READY = True\n",
    "print(\"[PATHS] Forced paths set and resolver overridden.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36891063-1790-4d63-a28b-c968a366eea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABS_DIR exists: True\n",
      "Files in ABS_DIR: ['sample_submission.csv', 'test.csv', 'train.csv']\n",
      "Exists train: True\n",
      "Exists test : True\n",
      "Exists samp : True\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "print(\"ABS_DIR exists:\", os.path.isdir(ABS_DIR))\n",
    "print(\"Files in ABS_DIR:\", sorted([os.path.basename(p) for p in glob.glob(os.path.join(ABS_DIR, \"*\"))]))\n",
    "print(\"Exists train:\", os.path.exists(TRAIN_PATH))\n",
    "print(\"Exists test :\", os.path.exists(TEST_PATH))\n",
    "print(\"Exists samp :\", os.path.exists(SAMP_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34509c0f-432b-464e-9263-89ee21c36423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 08:03:38) [Clang 14.0.6 ]\n",
      "NumPy : 1.26.4\n",
      "Pandas: 2.2.3\n",
      "📄 Using:\n",
      "  train.csv → /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/train.csv\n",
      "  test.csv  → /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/test.csv\n",
      "  sample    → /Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw/sample_submission.csv\n",
      "[NAMES] ID_COL=row_id | TARGET_OUT=rule_violation\n",
      "Train shape: (2029, 9)\n",
      "Test  shape: (10, 8)\n",
      "Sample shape: (10, 2)\n",
      "\n",
      "Models detected:\n",
      " - logreg_tfidf_feats\n",
      " - xgb_tfidf_feats\n",
      "\n",
      "OOF matrix shape: (2029, 3) | Test matrix shape: (10, 3)\n",
      "\n",
      "Model correlation matrix (OOF probs):\n",
      "[[1.    0.859]\n",
      " [0.859 1.   ]]\n",
      "\n",
      "[Mean] OOF macro-F1 = 0.7364 @ thr=0.483\n",
      "[Rank-Avg] OOF macro-F1 = 0.7391 @ thr=0.438\n",
      "\n",
      "Using val_f1-based weights: {'logreg_tfidf_feats': 0.5, 'xgb_tfidf_feats': 0.5}\n",
      "[Weighted-Avg] OOF macro-F1 = 0.7364 @ thr=0.483\n",
      "[LR Stacker] OOF macro-F1 = 0.7373 @ thr=0.468\n",
      " Meta weights (mean coef): {'logreg_tfidf_feats': 4.4948, 'xgb_tfidf_feats': 2.0943}\n",
      "\n",
      "=== WINNER: rank | OOF macro-F1=0.7391 @ thr=0.438 ===\n",
      "Winner confusion matrix:\n",
      " [[660 338]\n",
      " [187 844]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7792    0.6613    0.7154       998\n",
      "           1     0.7140    0.8186    0.7628      1031\n",
      "\n",
      "    accuracy                         0.7413      2029\n",
      "   macro avg     0.7466    0.7400    0.7391      2029\n",
      "weighted avg     0.7461    0.7413    0.7395      2029\n",
      "\n",
      "✅ Saved local copy : submissions/submission.csv\n",
      "\n",
      "Final summary:\n",
      " Winner: rank | OOF F1(macro)=0.7391 | thr=0.438\n",
      " First 5 submission rows:\n",
      "    row_id  rule_violation\n",
      "0    2029               0\n",
      "1    2030               1\n",
      "2    2031               1\n",
      "3    2032               1\n",
      "4    2033               1\n"
     ]
    }
   ],
   "source": [
    "# 05_ensemble.ipynb — Jigsaw Agile Community Rules\n",
    "# Purpose:\n",
    "#  - Load OOF (train) & test probabilities from prior notebooks\n",
    "#  - Explore blends: mean, rank-avg, weight-avg (optional weights from results/models.json)\n",
    "#  - Train a simple meta-learner (LogReg stacker) as Platt-like calibrator\n",
    "#  - Tune threshold (macro-F1) on OOF\n",
    "#  - Produce /kaggle/working/submission.csv and submissions/submission.csv\n",
    "\n",
    "# ========= 0) Imports & environment =========\n",
    "import os, sys, glob, json, math, warnings, re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"NumPy :\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "KAGGLE_DIR = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "KAGGLE_WORKING = \"/kaggle/working\" if IS_KAGGLE else None\n",
    "OUT_KAGGLE = os.path.join(KAGGLE_WORKING, \"submission.csv\") if IS_KAGGLE else None\n",
    "\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "OUT_LOCAL = \"submissions/submission.csv\"\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# ========= 1) Load core CSVs (Kaggle-first then local) =========\n",
    "\n",
    "# --- HARD-SET DATA PATHS (replacement for the old resolver cell) ---\n",
    "import os, pandas as pd\n",
    "\n",
    "ABS_DIR = \"/Users/michaelmaclennan/Documents/Learning & Education/2025-04 AI & ML/jigsaw-competition/data/raw\"\n",
    "\n",
    "TRAIN_PATH = os.path.join(ABS_DIR, \"train.csv\")\n",
    "TEST_PATH  = os.path.join(ABS_DIR, \"test.csv\")\n",
    "SAMP_PATH  = os.path.join(ABS_DIR, \"sample_submission.csv\")\n",
    "\n",
    "for p in (TRAIN_PATH, TEST_PATH, SAMP_PATH):\n",
    "    if not isinstance(p, (str, bytes, os.PathLike)) or not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Missing file: {p}\")\n",
    "\n",
    "print(\"📄 Using:\")\n",
    "print(\"  train.csv →\", TRAIN_PATH)\n",
    "print(\"  test.csv  →\", TEST_PATH)\n",
    "print(\"  sample    →\", SAMP_PATH)\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "sample   = pd.read_csv(SAMP_PATH)\n",
    "\n",
    "# Standardise names used later\n",
    "id_candidates = [c for c in sample.columns if (\"id\" in c.lower() or \"row\" in c.lower())]\n",
    "ID_COL = id_candidates[0] if id_candidates else sample.columns[0]\n",
    "TARGET_OUT = sample.columns[1] if sample.shape[1] >= 2 else \"prediction\"\n",
    "print(f\"[NAMES] ID_COL={ID_COL} | TARGET_OUT={TARGET_OUT}\")\n",
    "\n",
    "# Lock out any later resolvers\n",
    "KAGGLE_DIR = \"\"\n",
    "def _first_existing(paths):  # any later calls will return our forced paths\n",
    "    prefer = {\n",
    "        \"train.csv\": TRAIN_PATH,\n",
    "        \"test.csv\": TEST_PATH,\n",
    "        \"sample_submission.csv\": SAMP_PATH,\n",
    "    }\n",
    "    for p in paths:\n",
    "        base = os.path.basename(p)\n",
    "        if base in prefer:\n",
    "            return prefer[base]\n",
    "        if os.path.isabs(p) and os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "sample   = pd.read_csv(SAMP_PATH)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test  shape:\", test_df.shape)\n",
    "print(\"Sample shape:\", sample.shape)\n",
    "\n",
    "TEXT_COL = next((c for c in [\"comment_text\",\"body\",\"text\"] if c in train_df.columns), None)\n",
    "TARGET_COL = next((c for c in [\"rule_violation\",\"target\",\"label\"] if c in train_df.columns), None)\n",
    "ID_COL, TARGET_OUT = sample.columns[0], sample.columns[1]\n",
    "assert TARGET_COL, \"Target column not found.\"\n",
    "\n",
    "\n",
    "# ========= 2) Load OOF & test probabilities =========\n",
    "OOF_DIR  = \"results/oof\"\n",
    "TEST_DIR = \"results/test_probs\"\n",
    "oof_files  = sorted(glob.glob(os.path.join(OOF_DIR, \"*_oof.csv\")))\n",
    "test_files = sorted(glob.glob(os.path.join(TEST_DIR, \"*_test.csv\")))\n",
    "\n",
    "if len(oof_files) == 0 or len(test_files) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No OOF/test files found.\\n\"\n",
    "        f\"Expected OOF in {OOF_DIR}/*_oof.csv and test in {TEST_DIR}/*_test.csv \"\n",
    "        f\"with columns: row_id, prob\"\n",
    "    )\n",
    "\n",
    "def model_key_from_path(p):\n",
    "    base = os.path.basename(p)\n",
    "    # strip suffixes\n",
    "    return re.sub(r\"(_oof|_test)?\\.csv$\", \"\", base)\n",
    "\n",
    "# Map model key -> file path\n",
    "oof_map  = {model_key_from_path(p).replace(\"_oof\",\"\"): p for p in oof_files}\n",
    "test_map = {model_key_from_path(p).replace(\"_test\",\"\"): p for p in test_files}\n",
    "\n",
    "# Keep intersection only (models that have BOTH oof and test files)\n",
    "models = sorted(set(oof_map).intersection(set(test_map)))\n",
    "if len(models) < 2:\n",
    "    raise RuntimeError(f\"Need at least 2 models with both OOF and test probs; found: {models}\")\n",
    "\n",
    "print(\"\\nModels detected:\")\n",
    "for m in models:\n",
    "    print(\" -\", m)\n",
    "\n",
    "# Load & merge OOF by row_id aligned to train order\n",
    "train_ids = train_df[ID_COL].values\n",
    "y_true = train_df[TARGET_COL].astype(int).values\n",
    "\n",
    "oof_df = pd.DataFrame({ID_COL: train_ids})\n",
    "for m in models:\n",
    "    tmp = pd.read_csv(oof_map[m])\n",
    "    assert {\"row_id\",\"prob\"}.issubset(tmp.columns), f\"OOF file {oof_map[m]} missing columns.\"\n",
    "    oof_df = oof_df.merge(tmp.rename(columns={\"row_id\":ID_COL, \"prob\":f\"prob_{m}\"}), on=ID_COL, how=\"left\")\n",
    "\n",
    "missing_cols = [c for c in oof_df.columns if c.startswith(\"prob_\") and oof_df[c].isna().any()]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"OOF merge mismatch; NaNs in {missing_cols}. Ensure row_id alignment with train.csv\")\n",
    "\n",
    "# Load & merge Test by row_id aligned to sample order\n",
    "test_ids = sample[ID_COL].values\n",
    "test_df_probs = pd.DataFrame({ID_COL: test_ids})\n",
    "for m in models:\n",
    "    tmp = pd.read_csv(test_map[m])\n",
    "    assert {\"row_id\",\"prob\"}.issubset(tmp.columns), f\"Test file {test_map[m]} missing columns.\"\n",
    "    test_df_probs = test_df_probs.merge(tmp.rename(columns={\"row_id\":ID_COL, \"prob\":f\"prob_{m}\"}), on=ID_COL, how=\"left\")\n",
    "\n",
    "missing_cols_t = [c for c in test_df_probs.columns if c.startswith(\"prob_\") and test_df_probs[c].isna().any()]\n",
    "if missing_cols_t:\n",
    "    raise ValueError(f\"TEST merge mismatch; NaNs in {missing_cols_t}. Ensure row_id alignment with sample_submission.csv\")\n",
    "\n",
    "print(\"\\nOOF matrix shape:\", oof_df.shape, \"| Test matrix shape:\", test_df_probs.shape)\n",
    "\n",
    "# ========= 3) Quick diagnostics =========\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "probs_mat = oof_df[[c for c in oof_df.columns if c.startswith(\"prob_\")]].values\n",
    "corr = np.corrcoef(probs_mat, rowvar=False)\n",
    "print(\"\\nModel correlation matrix (OOF probs):\")\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(corr)\n",
    "\n",
    "# ========= 4) Blending strategies =========\n",
    "def macro_f1_at_threshold(y, p, thr):\n",
    "    pred = (p >= thr).astype(int)\n",
    "    return f1_score(y, pred, average=\"macro\")\n",
    "\n",
    "def tune_threshold(y, p, grid=None):\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.25, 0.75, 201)\n",
    "    f1s = [macro_f1_at_threshold(y, p, t) for t in grid]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(grid[i]), float(f1s[i])\n",
    "\n",
    "# 4a) Simple mean\n",
    "mean_oof = probs_mat.mean(axis=1)\n",
    "thr_mean, f1_mean = tune_threshold(y_true, mean_oof)\n",
    "print(f\"\\n[Mean] OOF macro-F1 = {f1_mean:.4f} @ thr={thr_mean:.3f}\")\n",
    "\n",
    "# 4b) Rank average (robust when scales differ)\n",
    "rank_oof = np.mean(np.argsort(np.argsort(probs_mat, axis=0), axis=0), axis=1) / (len(y_true)-1)\n",
    "thr_rank, f1_rank = tune_threshold(y_true, rank_oof)\n",
    "print(f\"[Rank-Avg] OOF macro-F1 = {f1_rank:.4f} @ thr={thr_rank:.3f}\")\n",
    "\n",
    "# 4c) Weighted average\n",
    "#    - If results/models.json exists and has val_f1 per model, use them as soft weights; else uniform.\n",
    "weights = None\n",
    "meta_path = \"results/models.json\"\n",
    "if os.path.exists(meta_path):\n",
    "    try:\n",
    "        meta = json.load(open(meta_path))\n",
    "        # meta can be list of dicts or dict keyed by model\n",
    "        valf1_by_model = {}\n",
    "        if isinstance(meta, list):\n",
    "            for r in meta:\n",
    "                if \"model\" in r and \"val_f1\" in r: valf1_by_model[r[\"model\"]] = r[\"val_f1\"]\n",
    "        elif isinstance(meta, dict):\n",
    "            for k,v in meta.items():\n",
    "                if isinstance(v, dict) and \"val_f1\" in v: valf1_by_model[k] = v[\"val_f1\"]\n",
    "        w = []\n",
    "        for m in models:\n",
    "            w.append(max(1e-6, float(valf1_by_model.get(m, 1.0))))\n",
    "        weights = np.array(w, dtype=float)\n",
    "        weights = weights / weights.sum()\n",
    "        print(\"\\nUsing val_f1-based weights:\", dict(zip(models, np.round(weights,4))))\n",
    "    except Exception as e:\n",
    "        print(\"Could not parse results/models.json; falling back to uniform weights.\", e)\n",
    "\n",
    "if weights is None:\n",
    "    weights = np.ones(len(models), dtype=float) / len(models)\n",
    "\n",
    "wavg_oof = np.average(probs_mat, axis=1, weights=weights)\n",
    "thr_wavg, f1_wavg = tune_threshold(y_true, wavg_oof)\n",
    "print(f\"[Weighted-Avg] OOF macro-F1 = {f1_wavg:.4f} @ thr={thr_wavg:.3f}\")\n",
    "\n",
    "# 4d) Logistic Regression stacker (acts as Platt-style calibrator too)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X_oof = probs_mat\n",
    "y_oof = y_true\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validated meta-probs to avoid bias\n",
    "meta_oof = np.zeros_like(y_oof, dtype=float)\n",
    "coefs = []\n",
    "for tr, va in skf.split(X_oof, y_oof):\n",
    "    clf = LogisticRegression(\n",
    "        solver=\"lbfgs\", max_iter=200, C=1.0, class_weight=\"balanced\", n_jobs=None\n",
    "    )\n",
    "    clf.fit(X_oof[tr], y_oof[tr])\n",
    "    meta_oof[va] = clf.predict_proba(X_oof[va])[:,1]\n",
    "    coefs.append(clf.coef_.ravel())\n",
    "\n",
    "thr_meta, f1_meta = tune_threshold(y_oof, meta_oof)\n",
    "coef_mean = np.mean(coefs, axis=0)\n",
    "print(f\"[LR Stacker] OOF macro-F1 = {f1_meta:.4f} @ thr={thr_meta:.3f}\")\n",
    "print(\" Meta weights (mean coef):\", dict(zip(models, np.round(coef_mean,4))))\n",
    "\n",
    "# ========= 5) Pick the winner on OOF =========\n",
    "candidates = [\n",
    "    (\"mean\", f1_mean, thr_mean, mean_oof),\n",
    "    (\"rank\", f1_rank, thr_rank, rank_oof),\n",
    "    (\"wavg\", f1_wavg, thr_wavg, wavg_oof),\n",
    "    (\"stacker\", f1_meta, thr_meta, meta_oof),\n",
    "]\n",
    "winner = sorted(candidates, key=lambda x: x[1], reverse=True)[0]\n",
    "WIN_NAME, WIN_F1, WIN_THR, WIN_OOF = winner\n",
    "print(f\"\\n=== WINNER: {WIN_NAME} | OOF macro-F1={WIN_F1:.4f} @ thr={WIN_THR:.3f} ===\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(\"Winner confusion matrix:\\n\", confusion_matrix(y_true, (WIN_OOF >= WIN_THR).astype(int)))\n",
    "print(classification_report(y_true, (WIN_OOF >= WIN_THR).astype(int), digits=4))\n",
    "\n",
    "# ========= 6) Build test predictions in the same way =========\n",
    "test_mat = test_df_probs[[c for c in test_df_probs.columns if c.startswith(\"prob_\")]].values\n",
    "\n",
    "if WIN_NAME == \"mean\":\n",
    "    test_probs = test_mat.mean(axis=1)\n",
    "elif WIN_NAME == \"rank\":\n",
    "    test_probs = np.mean(np.argsort(np.argsort(test_mat, axis=0), axis=0), axis=1) / (len(test_mat)-1)\n",
    "elif WIN_NAME == \"wavg\":\n",
    "    test_probs = np.average(test_mat, axis=1, weights=weights)\n",
    "else:\n",
    "    # Fit LR on full OOF to get final stacker, then apply to test\n",
    "    clf_full = LogisticRegression(solver=\"lbfgs\", max_iter=200, C=1.0, class_weight=\"balanced\")\n",
    "    clf_full.fit(X_oof, y_oof)\n",
    "    test_probs = clf_full.predict_proba(test_mat)[:,1]\n",
    "\n",
    "test_pred = (test_probs >= WIN_THR).astype(int)\n",
    "\n",
    "# ========= 7) Validate and write submission =========\n",
    "submission = sample.copy()\n",
    "submission[TARGET_OUT] = test_pred.astype(int)\n",
    "\n",
    "errors = []\n",
    "if list(submission.columns) != list(sample.columns):\n",
    "    errors.append(f\"Columns mismatch. Expected {list(sample.columns)}, got {list(submission.columns)}\")\n",
    "if len(submission) != len(sample):\n",
    "    errors.append(f\"Row count mismatch. Expected {len(sample)}, got {len(submission)}\")\n",
    "if not submission[ID_COL].equals(sample[ID_COL]):\n",
    "    if set(submission[ID_COL]) != set(sample[ID_COL]):\n",
    "        missing = list(sorted(set(sample[ID_COL]) - set(submission[ID_COL])))[:5]\n",
    "        extra   = list(sorted(set(submission[ID_COL]) - set(sample[ID_COL])))[:5]\n",
    "        errors.append(f\"ID set differs. Missing: {missing} | Extra: {extra}\")\n",
    "    else:\n",
    "        errors.append(\"ID order differs from sample. Must match sample_submission order.\")\n",
    "if submission[TARGET_OUT].isna().any():\n",
    "    errors.append(\"Target has NaNs.\")\n",
    "u = set(np.unique(submission[TARGET_OUT]))\n",
    "if not u.issubset({0,1}):\n",
    "    errors.append(f\"Target invalid values {sorted(u)}; must be 0/1.\")\n",
    "if errors:\n",
    "    print(\"❌ Submission invalid:\"); [print(\" -\", e) for e in errors]; raise SystemExit(1)\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    submission.to_csv(OUT_KAGGLE, index=False)\n",
    "    print(f\"✅ Saved Kaggle file: {OUT_KAGGLE}\")\n",
    "submission.to_csv(OUT_LOCAL, index=False)\n",
    "print(f\"✅ Saved local copy : {OUT_LOCAL}\")\n",
    "\n",
    "# ========= 8) Log run info =========\n",
    "run_info = {\n",
    "    \"task\": \"05_ensemble\",\n",
    "    \"time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"models\": models,\n",
    "    \"winner\": WIN_NAME,\n",
    "    \"oof_f1_macro\": float(WIN_F1),\n",
    "    \"threshold\": float(WIN_THR),\n",
    "}\n",
    "with open(\"results/run_05_ensemble.json\",\"w\") as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "\n",
    "print(\"\\nFinal summary:\")\n",
    "print(f\" Winner: {WIN_NAME} | OOF F1(macro)={WIN_F1:.4f} | thr={WIN_THR:.3f}\")\n",
    "print(\" First 5 submission rows:\\n\", submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edf868-648e-4e1f-9f55-871303e8fc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mac-py312)",
   "language": "python",
   "name": "tf-mac-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
